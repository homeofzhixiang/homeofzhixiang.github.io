<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>机器学习笔记——感知机（Perceptron)与自适性线性单元（Adaline)学习笔记 | Homeofzhixiang</title><meta name="author" content="沧月倾"><meta name="copyright" content="沧月倾"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="感知机（Perceptron)与自适性线性单元（Adaline)学习笔记说明：作为小白机器学习的笔记与记录，本博客主要参考借鉴以下书籍和文章：文章：  感知机原理（Perceptron） 统计学习方法|感知机原理剖析及实现 感知机（Perceptron) 感知机（原始形式和对偶形式）的理解和实现 机器学习 —— 基础整理（六）线性判别函数：感知器、松弛算法、Ho-Kashyap 算法 自适应线性单">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习笔记——感知机（Perceptron)与自适性线性单元（Adaline)学习笔记">
<meta property="og:url" content="http://homeofzhixiang.github.io/2023/01/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%84%9F%E7%9F%A5%E6%9C%BA-%E6%84%9F%E7%9F%A5%E6%9C%BA/index.html">
<meta property="og:site_name" content="Homeofzhixiang">
<meta property="og:description" content="感知机（Perceptron)与自适性线性单元（Adaline)学习笔记说明：作为小白机器学习的笔记与记录，本博客主要参考借鉴以下书籍和文章：文章：  感知机原理（Perceptron） 统计学习方法|感知机原理剖析及实现 感知机（Perceptron) 感知机（原始形式和对偶形式）的理解和实现 机器学习 —— 基础整理（六）线性判别函数：感知器、松弛算法、Ho-Kashyap 算法 自适应线性单">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://homeofzhixiang.github.io/img/cover/2.png">
<meta property="article:published_time" content="2023-01-23T16:38:53.046Z">
<meta property="article:modified_time" content="2023-01-24T10:23:07.081Z">
<meta property="article:author" content="沧月倾">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://homeofzhixiang.github.io/img/cover/2.png"><link rel="shortcut icon" href="/img/favicon/chicken1.png"><link rel="canonical" href="http://homeofzhixiang.github.io/2023/01/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%84%9F%E7%9F%A5%E6%9C%BA-%E6%84%9F%E7%9F%A5%E6%9C%BA/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"本文最后更新于","messageNext":"天前，其中的信息可能已经过时"},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":100,"languages":{"author":"作者: 沧月倾","link":"链接: ","source":"来源: Homeofzhixiang","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: true,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '机器学习笔记——感知机（Perceptron)与自适性线性单元（Adaline)学习笔记',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-01-24 18:23:07'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = url => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      link.onload = () => resolve()
      link.onerror = () => reject()
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/universe.css"><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.css" /><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> { preloader.endLoading() })

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar/1.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">3</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-book"></i><span> 学习</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/algorithm/"><i class="fa-fw fas fa-laptop"></i><span> 算法</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-bar-chart"></i><span> 科研</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/papernote/"><i class="fa-fw fas fa-sticky-note"></i><span> 笔记</span></a></li><li><a class="site-page child" href="/frontier/"><i class="fa-fw fas fa-compass"></i><span> 前沿追踪</span></a></li></ul></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/record/"><i class="fa-fw fas fa-pencil-square"></i><span> 记录</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas-envelope-open"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/cover/2.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Homeofzhixiang"><span class="site-name">Homeofzhixiang</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-book"></i><span> 学习</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/algorithm/"><i class="fa-fw fas fa-laptop"></i><span> 算法</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-bar-chart"></i><span> 科研</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/papernote/"><i class="fa-fw fas fa-sticky-note"></i><span> 笔记</span></a></li><li><a class="site-page child" href="/frontier/"><i class="fa-fw fas fa-compass"></i><span> 前沿追踪</span></a></li></ul></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/record/"><i class="fa-fw fas fa-pencil-square"></i><span> 记录</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas-envelope-open"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">机器学习笔记——感知机（Perceptron)与自适性线性单元（Adaline)学习笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-01-23T16:38:53.046Z" title="发表于 2023-01-24 00:38:53">2023-01-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-01-24T10:23:07.081Z" title="更新于 2023-01-24 18:23:07">2023-01-24</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">17k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>52分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="机器学习笔记——感知机（Perceptron)与自适性线性单元（Adaline)学习笔记"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="感知机（Perceptron-与自适性线性单元（Adaline-学习笔记"><a href="#感知机（Perceptron-与自适性线性单元（Adaline-学习笔记" class="headerlink" title="感知机（Perceptron)与自适性线性单元（Adaline)学习笔记"></a>感知机（Perceptron)与自适性线性单元（Adaline)学习笔记</h1><p>说明：作为小白机器学习的笔记与记录，本博客主要参考借鉴以下书籍和文章：<br>文章：</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/huangyc/p/9706575.html#_label1_2">感知机原理（Perceptron）</a></li>
<li><a target="_blank" rel="noopener" href="https://www.pkudodo.com/2018/11/18/1-4/">统计学习方法|感知机原理剖析及实现</a></li>
<li><a target="_blank" rel="noopener" href="https://michael.blog.csdn.net/article/details/104537083">感知机（Perceptron)</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/wkj1026639175/article/details/79827923?ops_request_misc=%25257B%252522request%25255Fid%252522%25253A%252522160856470616780277841266%252522%25252C%252522scm%252522%25253A%25252220140713.130102334..%252522%25257D&request_id=160856470616780277841266&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-4-79827923.nonecase&utm_term=%E6%84%9F%E7%9F%A5%E6%9C%BA%E5%AF%B9%E5%81%B6%E5%BD%A2%E5%BC%8F">感知机（原始形式和对偶形式）的理解和实现</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/Determined22/p/6507329.html">机器学习 —— 基础整理（六）线性判别函数：感知器、松弛算法、Ho-Kashyap 算法</a></li>
<li><a target="_blank" rel="noopener" href="https://max.book118.com/html/2019/0605/7166011156002031.shtm">自适应线性单元-机器学习系列</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/sanmaopep/article/details/78542361">李航第二章课后习题</a></li>
</ol>
<p>书籍：</p>
<ol>
<li>《统计学习方法》</li>
<li>《机器学习》</li>
<li>《神经网络设计》</li>
<li>《机器学习实战：基于Scikit-Learn、Keras和TensorFlow》</li>
<li>《深度学习入门：基于Python理论与实践》</li>
<li>《神经网络与深度学习》</li>
</ol>
<hr>
<p>##文章目录<br>&amp;emsp;&amp;emsp;<a href="#1">1.感知机简介</a><br>&amp;emsp;&amp;emsp;<a href="#2">2.感知机模型</a><br>&amp;emsp;&amp;emsp;<a href="#3">3.感知机学习策略</a><br>&amp;emsp;&amp;emsp;<a href="#4">4.最初的感知机算法与其问题</a><br>&amp;emsp;&amp;emsp;<a href="#5">5.自适性线性单元(ADALINE)及LMS算法</a><br>&amp;emsp;&amp;emsp;<a href="#6">6.统计学习方法中的感知机学习算法</a><br>&amp;emsp;&amp;emsp;&amp;emsp;<a href="#6.1">6.1 原始形式</a><br>&amp;emsp;&amp;emsp;&amp;emsp;<a href="#6.2">6.2 算法收敛</a><br>&amp;emsp;&amp;emsp;&amp;emsp;<a href="#6.3">6.3 对偶形式</a><br>&amp;emsp;&amp;emsp;&amp;emsp;<a href="#6.4">6.4 原始形式与对偶形式的简单比较与选择</a><br>&amp;emsp;&amp;emsp;<a href="#5">7.算法实现</a><br>&amp;emsp;&amp;emsp;&amp;emsp;<a href="#7.1">7.1 基于感知机 Perceptron 的鸢尾花分类实践</a><br>&amp;emsp;&amp;emsp;&amp;emsp;<a href="#7.2">7.2 基于感知机 Perceptron 的乳腺癌数据集分类</a></p>
<p>&amp;emsp;&amp;emsp;<a href="#8">8.总结思考</a><br>感知机的历史<br>线性回归与对规律的认知<br>数学角度的分类问题与感知机<br>MP神经元模型——逻辑运算的实现<br>联结主义对学习的理解<br>Rosenblatt 对记忆与感知的理解<br>感知机的学习观</p>
<hr>
<p><font size=5><center>正确的判断来自于经验，而经验来自于错误的判断。</center></font><br><font size=3><p align="right">弗雷德里克·布鲁克斯（Frederick P. Brooks）<br/>1999 年图灵奖获得者<br/>引自《神经网络与深度学习》</p><font></p>
<hr>
<hr>
<p>由于符号函数的不连续性，如果采用标准的均方误差，所得误差函数必然是不连续的，因而基于梯度的学习算法也就不能被使用。</p>
<hr>
<p>迭代思想</p>
<hr>
<h6 id='1'></h6>

<h2 id="1-感知机简介"><a href="#1-感知机简介" class="headerlink" title="1.感知机简介"></a>1.感知机简介</h2><h4 id="维基百科介绍"><a href="#维基百科介绍" class="headerlink" title="维基百科介绍:"></a>维基百科介绍:</h4><p>感知器（英语：Perceptron）是 Frank Rosenblatt 在 1957 年就职于康奈尔航空实验室（Cornell Aeronautical Laboratory）时所发明的一种人工神经网络。它可以被视为一种最简单形式的前馈神经网络，是一种二元线性分类器。</p>
<p>Frank Rosenblatt 给出了相应的感知机学习算法，常用的有感知机学习、最小二乘法和梯度下降法。譬如，感知机利用梯度下降法对损失函数进行极小化，求出可将训练数据进行线性划分的分离超平面，从而求得感知机模型。</p>
<p>感知机是生物神经细胞的简单抽象。神经细胞结构大致可分为：树突、突触、细胞体及轴突。单个神经细胞可被视为一种只有两种状态的机器——激动时为‘是’，而未激动时为‘否’。神经细胞的状态取决于从其它的神经细胞收到的输入信号量，及突触的强度（抑制或加强）。当信号量总和超过了某个阈值时，细胞体就会激动，产生电脉冲。电脉冲沿着轴突并通过突触传递到其它神经元。为了模拟神经细胞行为，与之对应的感知机基础概念被提出，如权量（突触）、偏置（阈值）及激活函数（细胞体）。<br><span><div style="text-align: center;"><br><img src="/%E7%94%9F%E7%89%A9%E7%A5%9E%E7%BB%8F.png" alt="生物神经"></p>
</div></span>

<p><span><div style="text-align: center;"><br><img src="/%E6%84%9F%E7%9F%A5%E6%9C%BA.png" alt="感知机"></p>
</div></span>

<p>在人工神经网络领域中，感知机也被指为单层的人工神经网络，以区别于较复杂的多层感知机（Multilayer Perceptron）。作为一种线性分类器，（单层）感知机可说是最简单的前向人工神经网络形式。尽管结构简单，感知机能够学习并解决相当复杂的问题。感知机主要的本质缺陷是它不能处理线性不可分问题。</p>
<h4 id="模型简述："><a href="#模型简述：" class="headerlink" title="模型简述："></a>模型简述：</h4><p>感知机（perceptron）是 二类分类的线性分类模型</p>
<ul>
<li>输入：实例的特征向量</li>
<li>输出：实例的类别，取 +1 和 -1 二值</li>
<li>类别：感知机对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面，属于判别模型</li>
<li>目标：求出将训练数据进行线性划分的分离超平面，为此，导入基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。</li>
<li>感知机学习算法具有简单而易于实现的优点，分为原始形式和对偶形式。</li>
<li>预测：对新的输入进行分类</li>
</ul>
<p>感知机 1957 年由 Rosenblatt（罗森布拉特）提出，是神经网络与支持向量机的基础。</p>
<hr>
<h6 id='2'></h6>

<h2 id="2-感知机的历史"><a href="#2-感知机的历史" class="headerlink" title="2. 感知机的历史"></a>2. 感知机的历史</h2><p>摘自<a target="_blank" rel="noopener" href="https://www.cnblogs.com/Determined22/p/6507329.html">机器学习 —— 基础整理（六）线性判别函数：感知器、松弛算法、Ho-Kashyap 算法</a></p>
<ul>
<li><p>1890 年，美国生物学家 W. James 出版了《Physiology》（生理学）一书，讲述有关人脑结构和功能，以及学习、记忆等基本规律，指出当两个基本处理单元同时活动，或两个单元靠得很近时，一个单元的兴奋状态会传递给另一单元，并且一个单元的活动程度与其周围的活动数量和密度成正比。</p>
</li>
<li><p>1943 年，Warren McCulloach, Walter Pitts 提出了著名的 MP 神经元模型，认为单元的输入存在兴奋边、抑制边两种，如果没有抑制边输入并且兴奋边的数量超过一个阈值，则神经元处于兴奋，否则处于抑制——这个模型给出了神经元的结构，使用阈值函数来输出，但所有输入边的权重都为 1，并且只要有一条抑制边输入那么就不可能兴奋。</p>
</li>
<li><p>1949 年，心理学家 Hebb 提出用于确定输入边权重的 Hebb 规则，该规则认为两个具有相同状态的神经元之间的权重要大。</p>
</li>
<li><p>1957 年，Frank Rosenblatt 提出感知器（Perceptron），随后在 IBM704 计算机上进行模拟从而证明了该模型有能力通过调整权重的学习达到正确分类的效果，第一次把神经网络从纯理论推到工程实践，掀起了研究高潮；在 1962 年感知器算法被证明是收敛的。</p>
</li>
<li><p>1960 年，Bernard Widrow 和 Ted Hoff 提出自适应线性单元（Adaptive linear neuron，ADALINE），学习规则也由 Hebb 规则转向 Delta 规则：利用实际输出与期望输出的误差来指导学习，即 LMS filter。</p>
</li>
<li><p>1969 年，Minsky 出版《Perceptron》，但同时指出感知器的两个问题：一个就是著名的“单层感知器无法解决异或（XOR，相异取真，相同取假）这样的线性不可分问题”，另外一个是这种局限在复杂结构中依然存在。不只是神经网络，整个 AI 都迎来寒冬。那时候晶体管还没有普及，大多是电子管（真空管），支持不了大规模计算。</p>
</li>
<li><p>1974 年，Paul Werbos 的博士论文里提出了 BP 算法（后被称为广义 Delta 规则），但是没有引起重视。<br>在 70 年代，据说全球只有几十个人在研究，但还是有一些成果：1976 年 Stephen Crossberg 的共振自适应理论（Adaptive resonance theory，ART 网络）、1979 年日本 Fukusima 的神经认知机（Neocognitron，将感受野应用到 NN）、80 年代芬兰 Kohonen 的自组织竞争神经网络等。</p>
</li>
<li><p>1982 年，物理学家 John Hopfield 提出 Hopfield 网络，基本思路是对于一个给定的神经网络，设计一个正比于每个神经元的活性和权重的能量函数，活性的改变算法向能量函数减小的方向进行，直到达到极小值。神经网络开始复兴。</p>
</li>
<li><p>1986 年，Rumelhart、Hinton、Williams 将 BP 应用到神经网络；1987 年，第一届世界神经网络大会在美国召开，千人参会。</p>
</li>
<li><p>1989 年，使用 sigmoid 激活的单隐层神经网络被证明可以逼近任意非线性函数；LeCun 的 CNN 用在文本识别；1990 年，RNN；1997 年，LSTM；2006 年，DBN，深度学习的概念被提出；2011 年，语音识别领域首先取得突破；2012 年，AlexNet 在 ImageNet 夺冠，引爆这个领域；2016 年，Alphago战胜世界围棋冠军李世石······</p>
</li>
</ul>
<h3 id="如何学习？"><a href="#如何学习？" class="headerlink" title="如何学习？"></a>如何学习？</h3><hr>
<h6 id='3'></h6>

<h2 id="3-线性回归与对规律的认知"><a href="#3-线性回归与对规律的认知" class="headerlink" title="3. 线性回归与对规律的认知"></a>3. 线性回归与对规律的认知</h2><h3 id="线性回归——函数近似机-x2F-逼近器"><a href="#线性回归——函数近似机-x2F-逼近器" class="headerlink" title="线性回归——函数近似机&#x2F;逼近器"></a>线性回归——函数近似机&#x2F;逼近器</h3><p>给出两个点我们就能确定一条线，即一个线性函数。进而我们可以用这个线性函数判断其他任意给出的点是否在此直线上。这一过程可以抽象一般化，理解为我们可以直接通过具体实例来倒推出一般函数，并可借助该函数进行判断和预测。在统计学中，这种类似的方法被称为线性回归。<br>所谓线性回归，是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。简单来说，线性回归就是指从几对输入值(x)和对应输出值(y)的实例中概括出一个一般函数。<br>线性回归是一个有着两百年历史的从一些输入输出对组中推断出一般函数的技巧。<strong>它的价值和意义在于很多函数难以给出明确的方程表达，但是，却很容易在现实世界搜集到输入值和输出值实例</strong>——比如，将说出来的词的音频作为输入，词本身作为输出的映射函数。</p>
<h3 id="由线性回归思考何为规律？"><a href="#由线性回归思考何为规律？" class="headerlink" title="由线性回归思考何为规律？"></a>由线性回归思考何为规律？</h3><h4 id="对规律性存在的假设"><a href="#对规律性存在的假设" class="headerlink" title="对规律性存在的假设"></a>对规律性存在的假设</h4><p>线性回归之所以可行，是因为假设存在一个线性模型作为规则决定着所有样本数据，并可通过具体数据逆推拟合出该线性模型。<br>线性回归可以抽象出一个重要的认知启示，即世界万物是存在其规律性的，规律无处不在，并蕴含在具体实例中、借由具体实例展现。</p>
<h4 id="规律的本质"><a href="#规律的本质" class="headerlink" title="规律的本质"></a>规律的本质</h4><p>规律的本质是函数，即不同事物间的映射关系</p>
<h4 id="规律的学习"><a href="#规律的学习" class="headerlink" title="规律的学习"></a>规律的学习</h4><p>规律不是虚无的，规律蕴含在具体特例中。因而规律是可以被归纳学习的，可以从具体的例子中学习。</p>
<hr>
<h6 id='4'></h6>


<h2 id="数学角度的分类问题与感知机"><a href="#数学角度的分类问题与感知机" class="headerlink" title="数学角度的分类问题与感知机"></a>数学角度的分类问题与感知机</h2><p>模式识别</p>
<h3 id="对分类问题的认知"><a href="#对分类问题的认知" class="headerlink" title="对分类问题的认知"></a>对分类问题的认知</h3><p>分类问题其实就是模式识别。我会容易将分类与比较混淆，但其实比较只是分类的方法而不是本质。分类的本质在于“识别”，在于研究并把握事物所具有的特有的属性的规律。事实上，我们在分类时，首先通过样例抽象总结出事物各属性特征的规律并建立对应的“类”的模型，当我们对新的个体分类时，我们将其特征与抽象模型相对比，进而将其分类。在此意义上，“比较”只是为了在抽象模型时能抓住核心属性特征而使用的方法，而不是“分类”本身。分类的核心是对特征规律——“模式”的识别与建模（表示）过程。<br>例如，图像识别是一种分类，从猫狗识别到人脸识别到自动驾驶中的交通标志检测都属于“认知”并“判断”的分类问题。<br><strong>因而分类可被理解为对事物的某一或某些性质进行研究，挖掘对比其中的规律性不同并进行划分。</strong><br>在机器学习中，这意味着寻找<strong>决策边界</strong>。</p>
<p>为什么要分类？<br>我们分类除了为了认知世界&#x2F;事物，更重要的是改造世界&#x2F;事物，是为了应用。要想理解分类问题，我们就要搞懂分类的目的与需求。<strong>实际上我们之所以分类是为了针对不同种类事物具有的特定规律&#x2F;特征，对它们采取不同的应对和处理策略。我们不仅对事物的类别本身进行抽象建模，我们也对其对应的特有的应对和处理策略建立对应的模式行为。这是因为不同事物由于其特征不同，其价值、功能、用途、需求等也对应着不同。</strong></p>
<p>在机器学习中，广告点击行为预测、O2O优惠券使用预测、基于文本内容的垃圾短信识别等应用就是为了更好个性化应对不同类别的人&#x2F;事物而进行的分类问题。</p>
<h3 id="数学角度的分类问题"><a href="#数学角度的分类问题" class="headerlink" title="数学角度的分类问题"></a>数学角度的分类问题</h3><p>超平面。</p>
<h4 id="何为函数"><a href="#何为函数" class="headerlink" title="何为函数"></a>何为函数</h4><h3 id="数学建模"><a href="#数学建模" class="headerlink" title="数学建模"></a>数学建模</h3><p>所谓模型是指为了某个特定目的将原型的某一部分信息简缩、提炼而构造的原型替代物。可以发现，为了研究事物与问题，我们往往必须通过建模的方式来处理复杂信息、了解事物。建模可谓是研究实际问题的必备方法。</p>
<p>而数学作为精确描述数量关系、空间形式的语言、逻辑推理的载体，是精确研究事物的必要条件。因而，我们需要在研究问题时建立适当的数学模型，将实际问题与数学工具连接起来。一般地说，数学建模可以描述，对于现实世界的一个特定对象，为了一个特定目的，根据特有的内在规律，做出一些必要的简化假设，运用适当的数学工具，得到的一个数学结构。</p>
<p>数学建模中两种基本方法：机理分析与测试分析。机理分析是根据对客观事物特性的认识，找出反应内部机理的数量规律，建立的模型常有明确的物理或现实意义。<strong>测试分析是将研究对象看作一个“黑箱”系统（意思是它的内部机理看不清楚），通过对系统输入、输出数据的测量和统计分析，按照一定的准则找出与数据拟合得最好的模型</strong>。</p>
<p>对于许多实际问题还常常将两种方法结合起来建模，即<strong>用机理分析建立模型的结构，用测试分析确定模型的参数</strong>。</p>
<p>上述内容大多摘自《数学模型》一书。我觉得非常有趣的是，这基本就是在描述机器学习中的构建方法。</p>
<h3 id="从数学建模角度理解感知机"><a href="#从数学建模角度理解感知机" class="headerlink" title="从数学建模角度理解感知机"></a>从数学建模角度理解感知机</h3><p>正如上文所说机器学习中所谓模型的构建过程基本上就是数学建模的过程——通过机理分析建立模型的结构，通过测试分析确定模型的参数。只不过在数学建模中，我们是通过最小二乘法、凸优化的人工进行拟合，而在机器学习中我们设计优化算法，让计算机自己进行拟合和参数确定。</p>
<p>在此意义上，感知机不过是通过机理分析确定一个线性超平面模型，并设计了一个学习规则&#x2F;优化算法，借助训练实例，让计算机来确定超平面的权重和偏置参数。</p>
<hr>
<h6 id='4'></h6>

<h2 id="4-MP-神经元模型——逻辑运算的实现"><a href="#4-MP-神经元模型——逻辑运算的实现" class="headerlink" title="4. MP 神经元模型——逻辑运算的实现"></a>4. MP 神经元模型——逻辑运算的实现</h2><p>1943 年，沃伦·麦卡洛克（Warren McCulloch）和沃尔特·皮茨（Walter Pitts）在研究生物神经元时指出，具有二进制阈值激活功能的神经元能够实现一阶逻辑推理，并抽象出 MP 神经元模型。</p>
<h4 id="原始的-MP-神经元模型"><a href="#原始的-MP-神经元模型" class="headerlink" title="原始的 MP 神经元模型"></a>原始的 MP 神经元模型</h4><p>早期的 MP 神经元具有一个或多个二进制（开&#x2F;关）输入和一个二进制输出。当超过一定数量的输入处于激活状态时，人工神经元将激活其输出。McCulloch 和 Pitts 的论文表明即使使用这样的简化模型，也可以构建一个人工神经元来计算所需的任何逻辑命题。</p>
<p><span><div style="text-align: center;"></p>
<p><img src="/%E6%97%A9%E6%9C%9FMP%E7%A5%9E%E7%BB%8F%E5%85%83.png" alt="早期MP神经元.png"></p>
</div></span>

<p>例如，如上图，我们可以构建一些神经元，假设神经元至少有两个输入处于激活状态时，神经元就会被激活，则我们可实现多种逻辑运算：</p>
<ol>
<li>左边的第一个网络是一个简单的恒等函数：如果神经元 A 被激活，那么神经元 C 也被激活（因为它从神经元 A 接受到两个输入信号），如果神经元 A 关闭，那么神经元 C 也关闭。<br>值得注意的是，在这里，每个神经元可以有多个信号相同的输出。</li>
<li>第二个网络计算逻辑“与”（AND)：只有当神经元 A 和 B 都处于激活状态，神经元 C 才会激活（单独的一个输入信号并不足以激活神经元 C）。</li>
<li>第三个网络计算逻辑“或”（OR)：如果神经元 A 或 B 中有一个（或者两者都）处于激活状态时，神经元 C 就会被激活。</li>
<li>最后，如果我们假设输入连接可以抑制神经元的活动（正如生物神经网络中那样），那么第四个网络计算的就是一个比较复杂的逻辑命题：只有在神经元 A 是激活而且神经元 B 是关闭时，神经元 C 才会处于激活状态。如果神经元 A 一直处于激活状态，那你就得到了逻辑“非”（NOT)：神经元关闭时神经元 C 处于激活状态，反之亦然。</li>
</ol>
<h4 id="原始-MP-神经元的问题——权重的缺失"><a href="#原始-MP-神经元的问题——权重的缺失" class="headerlink" title="原始 MP 神经元的问题——权重的缺失"></a>原始 MP 神经元的问题——权重的缺失</h4><p>早期 MP 神经元没有引入“权重概念”，模型过于简单、只能输入相同的激活信号或抑制信号并结合阈值来控制，这显然是无法解决复杂问题的，因为究其本质，它不过只能处理一些离散的、整数形式的简单加减运算而已。由于信号全是相同的，只能通过调节输入数量来调节输出，这显然要预先人工设计且没什么实际功用。</p>
<p>因而，我们必须引入每个指标对不同输入信号进行区别对待与处理，使每个信号都不同。</p>
<h4 id="引入权重概念的-MP-神经元"><a href="#引入权重概念的-MP-神经元" class="headerlink" title="引入权重概念的 MP 神经元"></a>引入权重概念的 MP 神经元</h4><p>伴随着感知机的出现，MP 神经元被引入权重概念。在这个模型中，神经元接收到来自 n 个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接(connection)进行传递神经元接收到的总输入值将与神经元的阙值进行比较，然后通过 “激活函数” (activation function)处理以产生神经元的输出。MP 神经元采用的激活函数是阶跃函数，它将输入值映射为输出值“0”或“1”,显然“1”对应于神经元兴奋，“0”对应于神经元抑制。</p>
<p>引入权重概念是具有实际意义的，一方面通过权重每个输入信号都是独特的，并且不再只能取“激活”“抑制”两个定性整数值，每个信号都能定量化为任意大小的数值，这种差异性、数值的任意性才能用来表示各种不同的信息；同时权重起到了对信号重要性的评估作用，以及控制信号流动难度和信息处理、筛选、压缩的作用。</p>
<p><span><div style="text-align: center;"></p>
<p><img src="/MP%E7%A5%9E%E7%BB%8F%E5%85%83.png" alt="MP神经元"></p>
</div></span>

<p>引入权重的 MP 神经元模型能更方便的实现一阶逻辑“与”“或”“非”运算，如：</p>
<ul>
<li>逻辑“与”($x_1\land x_2$)：令$\omega_1&#x3D;\omega_2&#x3D;1,\theta&#x3D;2$，则$y&#x3D;f(1\cdot x_1+1\cdot x_2-2)$，仅在$x_1&#x3D;x_2&#x3D;1$时，$y&#x3D;1$；<br/></li>
<li>逻辑“或”($x_1\lor x_2$)：令$\omega_1&#x3D;\omega_2&#x3D;1,\theta&#x3D;0.5$，则$y&#x3D;f(1\cdot x_1+1\cdot x_2-0.5)$，$x_1&#x3D;1$或$x_2&#x3D;1$时，$y&#x3D;1$；<br/></li>
<li>逻辑“非”($x_1\lnot x_2$)：令$\omega_1&#x3D;-0.6,\omega_2&#x3D;0,\theta&#x3D;-0.5$，则$y&#x3D;f(-0.6\cdot x_1+0\cdot x_2+0.5)$，当$x_1&#x3D;1$时$y&#x3D;0$；当$x_1&#x3D;0$时，$y&#x3D;1$；</li>
</ul>
<h3 id="MP-神经元的问题——逻辑运算并不能实现学习"><a href="#MP-神经元的问题——逻辑运算并不能实现学习" class="headerlink" title="MP 神经元的问题——逻辑运算并不能实现学习"></a>MP 神经元的问题——逻辑运算并不能实现学习</h3><p>在人工智能的早期时代，主流思想认为，如果计算机能够做正式的逻辑推理，则将本质上解决人工智能问题。然而，事实证明，只实现简单逻辑推理并不能解决智能问题。所谓逻辑运算不过是简单的离散线性运算而已，由于权重的值都是预先设置的，因而这种线性运算只能用于预先人工设置好权重值的特定问题，没有推广能力。</p>
<p>简单来说，权重固定的 MP 神经元不具有学习能力。</p>
<p>而为了让神经元具有学习能力，我们应该思考的是学习为何、学习有哪些方法、我们通过何种方法能表征学习的过程。下面我们将介绍1949年Hebb对这一问题的一些见解以及联结主义思想。</p>
<hr>
<h6 id='5'></h6>

<h2 id="5-如何学习——联结主义对学习的理解"><a href="#5-如何学习——联结主义对学习的理解" class="headerlink" title="5. 如何学习——联结主义对学习的理解"></a>5. 如何学习——联结主义对学习的理解</h2><p>联结主义是 20 世纪初美国心理学家 E.L.桑代克在对动物实验研究的基础上提出的一种学习心理学理论。认为情境感觉和动作冲动反应之间形成的联结是学习的基础，也是心理行为的基本单位。“联结”一词，意指实验动物对笼内情境的感觉和反应动作的冲动之间形成的联系或联想。</p>
<p>在联结主义中，Hebb的观点直接启发了感知机的诞生。</p>
<h4 id="桑代克的研究"><a href="#桑代克的研究" class="headerlink" title="桑代克的研究"></a>桑代克的研究</h4><p>桑代克的联结主义以经验论为理论基础。</p>
<p>桑代克认为，联结是行为的基本单元。他把行为区分为先天的反应趋势和习得的反应趋势两类：前一类的反应趋势或联结主要是本能，后一类的反应趋势或联结主要是习惯。本能是不经学习的先天联结，习惯是后天习得的联结。</p>
<p>桑代克根据对动物学习实验的研究，认为学习的基本方式是尝试错误式的学习。动物由多次“尝试错误与偶然成功”形成联结而完成学习；桑代克并具体提出练习律、效果律、准备律等学习定律(参见“试误学习”)。桑代克对这些学习定律进行了修订和补充。他提出“相属原则”修订练习律，认为相属的容易造成联结，不相属的不容易造成联结。另外指出赏和惩的效果并不相等，奖赏比惩罚更为有效，这就用奖赏补充了效果律。</p>
<h4 id="赫布规则"><a href="#赫布规则" class="headerlink" title="赫布规则"></a>赫布规则</h4><p>1949 年， 心理学家唐纳德·赫布（Donald Hebb）在研究神经元的基础上提出了一个关于学习观念的出人意料并影响深远的想法。在他的《行为的组织》一书中，赫布声称知识和学习发生在大脑主要是依靠神经元间突触的形成与变化，简要表述为赫布规则：</p>
<p><strong>当细胞 A 的轴突足以接近以激发细胞 B，并反复持续地对细胞放电，一些生长过程或代谢变化将发生在某一个或这两个细胞内（如激活阈值减小，联系增强），以致 A 作为对 B 放电的细胞中的一个，效率增加。</strong></p>
<p>赫布认为这种活动是学习和记忆所必需的基本操作之一。</p>
<h4 id="赫布规则的启示"><a href="#赫布规则的启示" class="headerlink" title="赫布规则的启示"></a>赫布规则的启示</h4><p>赫布规则认为学习与记忆不是直接映射性的编码，而是储存在网络的连接中。</p>
<p>赫布规则同时也启示了神经网络中的Hebb学习规则，可用于无监督学习和有监督学习。</p>
<h4 id="认知心理学中的联结主义"><a href="#认知心理学中的联结主义" class="headerlink" title="认知心理学中的联结主义"></a>认知心理学中的联结主义</h4><p>20 世纪 80 年代初，认知心理学中兴起的一种认知研究范式，亦即网络模型。联结主义的指导性启示和主要灵感来自大脑或神经系统，<strong>它把认知看成是网络的整体活动</strong>。网络是个动态的系统，它由类似于神经元的基本单元和结点构成，每个单元都有不同的活性。随着时间的衰减，外部输入和其他单元的活性传递都会使一个单元的静息活性发生动态的改变。联结主义赋予网络以核心性的地位，采纳<strong>分布表征和并行加工理论</strong>，强调的是网络的并行分布加工，注重的是网络加工的数学基础。20 世纪 80 年代以来，网络取向的联结主义取代了符号取向的认知主义，成为现代认知心理学的理论基础。</p>
<p>关于赫布理论和联结主义的更多内容可见以下链接：</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E8%B5%AB%E5%B8%83%E7%90%86%E8%AE%BA/8347084?fr=aladdin">百度百科 赫布理论</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/89446215">长文干货！联结主义大百科</a></li>
<li><a target="_blank" rel="noopener" href="https://baijiahao.baidu.com/s?id=1633799033897948093&wfr=spider&for=pc">符号主义和联结主义 AI，都是什么鬼？</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/20648971">顾险峰：人工智能中的联结主义和符号主义</a></li>
<li><a target="_blank" rel="noopener" href="http://www.360doc.com/content/16/0613/12/16295112_567379244.shtml">认知神经的可塑性：赫布理论的哲学意蕴</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/19939960">三大神经科学基本定律</a></li>
</ol>
<hr>
<h6 id='5'></h6>

<h2 id="5-Rosenblatt-对记忆与感知的理解"><a href="#5-Rosenblatt-对记忆与感知的理解" class="headerlink" title="5. Rosenblatt 对记忆与感知的理解"></a>5. Rosenblatt 对记忆与感知的理解</h2><p>罗森布拉特在 1957 年的论文中开篇便以比较的方式阐述了他对记忆与感知的基于生物学假设的认知。</p>
<p>由于本人英语和生物学、数学水平浅薄，对该论文只进行了粗略阅读，但仍很受启发。在这里，我简单列举出其中简单内容，完整内容见《Rosenblatt, Frank. x. (1958), The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain》</p>
<h4 id="关于记忆与感知的两种观点————硬式编码与联结主义"><a href="#关于记忆与感知的两种观点————硬式编码与联结主义" class="headerlink" title="关于记忆与感知的两种观点————硬式编码与联结主义"></a>关于记忆与感知的两种观点————硬式编码与联结主义</h4><p>在论文开篇，Rosenblatt 首先提出了三个问题：</p>
<ol>
<li>物质世界的信息是如何被生物系统感知或探测到的？</li>
<li>信息以何种形式储存或记忆？</li>
<li>储存或记忆中的信息是如何影响认知和行为的？<br>他针对第二、三问列举了当时两个不同立场的观点：</li>
</ol>
<ul>
<li>第一种观点是机械式的编码记忆与硬式存储，认为感官信息的储存是以编码形式或图像的形式存在的，在感官刺激之间存在某种一对一映射关系和存储模式，因而原则上这种记忆是可以进行还原、重建与复制的。围绕这一假设，当时发展出围绕编码、符号逻辑的大脑模型。表征记忆的概念发展起来。<br>这一观点必然得出这样的结论：对任何刺激的识别都涉及到将存储的内容与进入的感觉模式进行匹配或系统的比较，以便确定当前的刺激是否在之前被看到过，并确定机体的适当反应。因而这种识别需要一个单独的系统与进程。</li>
<li>另一种观点则是联结主义。联结主义认为感官刺激的信息或图片可能并没有被真正直接记录和存储。信息被包含在连接或关联中。刺激不会被简单地映射到记忆中，信息被以某种方式存储为对特定响应地偏好。而新的刺激会在原有连接基础上自动激活适当的响应、形成新的连接而不需要一个单独的系统与进程进行识别。因而，在这一观点下，智能系统的识别功能的一般化、普适化的，而不局限于特定的、需要已知的条件。</li>
</ul>
<h4 id="Rosenblatt-的观点"><a href="#Rosenblatt-的观点" class="headerlink" title="Rosenblatt 的观点"></a>Rosenblatt 的观点</h4><p>Rosenblatt 的感知机模型采用后一种观点。用他的原话说：“感知机的设计是为了说明一般智能系统的一些基本特性，而不是过于深入地沉浸在特殊的、通常是未知的条件中。”</p>
<p>他把感知机描述为“在一个随机刺激的环境中，一个由随机连接的单元组成的系统，受上面讨论的参数约束，可以学会将特定的反应与特定的刺激联系起来。即使许多刺激与每一种反应相关，它们仍然可以被更好的概率识别，尽管它们可能彼此非常相似，并可能激活许多相同的感官输入到系统。”</p>
<p>他把感知机的学习归结为一种建立在概率基础上的迭代试错学习。</p>
<h4 id="采用概率语言而非符号逻辑"><a href="#采用概率语言而非符号逻辑" class="headerlink" title="采用概率语言而非符号逻辑"></a>采用概率语言而非符号逻辑</h4><p>感知机模型构建的一个问题是表示语言的选用。当时神经网络研究的一个主要问题是对于一个包含许多随机连接的不完善的神经网路，如何能够可靠地执行那些可能由理想化的接线图所表示的功能。然而当时研究人工智能最常用的符号逻辑和布尔代数语言不太适合这样的研究。需要一种合适的语言来对系统中的事件进行数学分析。在这些系统中，只有总体的组织可以被描述出来，而精确的结构是未知的，这导致 Rosenblatt 选用概率论而不是符号逻辑来制定感知机模型。<br>（不同于现在我们学到的感知机算法，在他当时的论文中，Rosenblatt 是使用概率来解释感知机的。之所以强调这一点，只因为这恰恰呼应了后来统计学习方法相关理论的发展，采用概率构建学习模型。这是很有趣的。）</p>
<hr>
<h6 id='6'></h6>

<h2 id="6-感知机的学习观"><a href="#6-感知机的学习观" class="headerlink" title="6. 感知机的学习观"></a>6. 感知机的学习观</h2><h3 id="感知机背后涉及的一些学习方法"><a href="#感知机背后涉及的一些学习方法" class="headerlink" title="感知机背后涉及的一些学习方法"></a>感知机背后涉及的一些学习方法</h3><h4 id="归纳推理——基于例子的学习"><a href="#归纳推理——基于例子的学习" class="headerlink" title="归纳推理——基于例子的学习"></a>归纳推理——基于例子的学习</h4><p>归纳推理是一种由个别到一般的推理。由一定程度的关于个别事物的观点过渡到范围较大的观点，由特殊具体的事例推导出一般原理、原则的解释方法。自然界和社会中的一般，都存在于个别、特殊之中，并通过个别而存在。一般都存在于具体的对象和现象之中，因此，只有通过认识个别，才能认识一般。人们在解释一个较大事物时，从个别、特殊的事物总结、概括出各种各样的带有一般性的原理或原则，然后才可能从这些原理、原则出发，再得出关于个别事物的结论。这种认识秩序贯穿于人们的解释活动中，不断从个别上升到一般，即从对个别事物的认识上升到对事物的一般规律性的认识。例如，根据各个地区、各个历史时期生产力不发展所导致的社会生活面貌落后，可以得出结论说，生产力发展是社会进步的动力，这正是从对于个别事物的研究得出一般性结论的推理过程，即归纳推理。显然，归纳推理是从认识研究个别事物到总结、概括一般性规律的推断过程。在进行归纳和概括的时候，解释者不单纯运用归纳推理，同时也运用演绎法。在人们的解释思维中，归纳和演绎是互相联系、互相补充、不可分割的。<br><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%BD%92%E7%BA%B3%E6%8E%A8%E7%90%86?fromtitle=%E5%BD%92%E7%BA%B3%E6%B3%95&fromid=120098">百度百科链接</a></p>
<h4 id="试错法——逐步调整"><a href="#试错法——逐步调整" class="headerlink" title="试错法——逐步调整"></a>试错法——逐步调整</h4><p>通过不断试验和消除误差，探索具有黑箱性质的系统的方法。这种方法在动物的行为中是不自觉地应用的，在人的行为中则是自觉的。试错法是纯粹经验的学习方法。应用试错法的主体通过间断地或连续地改变黑箱系统的参量，试验黑箱所作出的应答，以寻求达到目标的途径。主体行为的成败是用它趋近目标的程度或达到中间目标的过程评价的。趋近目标的信息给主体，主体就会继续采取成功的行为方式；偏离目标的信息反馈给主体，主体就会避免采取失败的行为方式。通过这种不断的尝试和不断的评价，主体就能逐渐达到所要追求的目标。<br><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E8%AF%95%E9%94%99%E6%B3%95">百度百科链接</a></p>
<h4 id="迭代法——重复执行，逐步优化"><a href="#迭代法——重复执行，逐步优化" class="headerlink" title="迭代法——重复执行，逐步优化"></a>迭代法——重复执行，逐步优化</h4><p>迭代法也称辗转法，是一种不断用变量的旧值递推新值的过程，跟迭代法相对应的是直接法(或者称为一次解法)，即一次性解决问题。迭代算法是用计算机解决问题的一种基本方法，它利用计算机运算速度快、适合做重复性操作的特点，让计算机对一组指令(或一定步骤)进行重复执行，在每次执行这组指令(或这些步骤)时，都从变量的原值推出它的一个新值，迭代法又分为精确迭代和近似迭代。比较典型的迭代法如“二分法”和”牛顿迭代法”属于近似迭代法。<br><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E8%BF%AD%E4%BB%A3%E6%B3%95/10913188">百度百科链接</a></p>
<h3 id="感知机的大胆假设"><a href="#感知机的大胆假设" class="headerlink" title="感知机的大胆假设"></a>感知机的大胆假设</h3><p>我们如果能够掌握大量例子，我们就能掌握规律本身。而掌握规律可以用试错的方法进行。概率中的大数定理等都映照着这种假设。照应于前文，显然，这种假设结合了经验主义、归纳学习、迭代试错逐步调整等思想。</p>
<p>这个假设使我认为感知机所隐含的很基础的不足为奇的假设。然而作为初学者，这一观点对于我仍是很有趣的，因为我从没有思考过相关的认知的问题，并且就像费曼所说的那样，太多太多时候我总陷入概念的世界里而并不了解背后的真实事物本身及其内在机理。</p>
<p>另一方面，这一假设显然是有问题的。比如在学习EM算法时我就了解到还有“隐变量”这一概念，显然，直接学习大量例子是不能很好应对这一问题的，换句话说学习实例也是需要相关的数学方法与技巧的。又如例子的代表性、先验分布等。</p>
<p>但我仍觉得这一观点是值得强调的。</p>
<h3 id="感知机与传统方法的区别"><a href="#感知机与传统方法的区别" class="headerlink" title="感知机与传统方法的区别"></a>感知机与传统方法的区别</h3><p>传统方法试图直接求解函数的参数而感知机以迭代的方法让计算机自己学习函数的参数</p>
<h3 id="试错的方法与试错的效率问题"><a href="#试错的方法与试错的效率问题" class="headerlink" title="试错的方法与试错的效率问题"></a>试错的方法与试错的效率问题</h3><hr>
<h6 id='7'></h6>

<h2 id="7-感知机模型"><a href="#7-感知机模型" class="headerlink" title="7.感知机模型"></a>7.感知机模型</h2><p>感知机定义：</p>
<ul>
<li>输入空间： $X\subseteq  R^n$</li>
<li>输出模型： $Y &#x3D; {+1,-1}$</li>
<li>$x \subset X$表示实例的特征向量，$y \subset Y$表示实例类别</li>
<li>输入空间到输出空间的函数（感知机模型）：<br>$$f(x) &#x3D; sign(\omega\cdot x + b)$$</li>
<li>参数：$\omega权重向量，b偏置$</li>
<li>$sign()$是符号函数,属于阶跃函数：<br>$$sign(x) &#x3D;<br>\begin{cases}<br>+1, x \geq 0 \<br>-1, x &lt; 0<br>\end{cases}<br>$$</li>
<li>感知机是线性分类模型，判别模型</li>
<li>几何解释：线性方程$$\omega\cdot x + b$$对应 n 为空间的一个超平面，$\omega$是其法向量，b 是其截距。<br>超平面将特征向量划分为两个部分，位于两部分的点（特征向量）分别被分成正、负两类<br><span><div style="text-align: center;"><br><img src="/%E8%B6%85%E5%B9%B3%E9%9D%A2.png" alt="超平面"></div></span></li>
</ul>
<p>####问题思考<br><a href="">对于超平面的理解？</a></p>
<hr>
<h6 id='3'></h6>

<h2 id="3-感知机学习策略"><a href="#3-感知机学习策略" class="headerlink" title="3.感知机学习策略"></a>3.感知机学习策略</h2><h4 id="数据集的线性可分行："><a href="#数据集的线性可分行：" class="headerlink" title="数据集的线性可分行："></a>数据集的线性可分行：</h4><ul>
<li>给定数据集$T &#x3D; {(x_1,y_1),(x_2,y_2)\cdots(x_N,y_N)}$,如果存在一个超平面$$\omega\cdot x + b$$能够将数据集的所有正实例点和负实例点完全正确地划分在平面两侧，则其线性可分，称 T 为线性可分数据集，否则线性不可分</li>
<li>感知机只能处理线性可分问题 ####感知机学习的假设</li>
<li>感知机学习的重要前提假设是训练数据集是线性可分的。</li>
</ul>
<h4 id="感知机的学习策略"><a href="#感知机的学习策略" class="headerlink" title="感知机的学习策略"></a>感知机的学习策略</h4><p>感知机的学习目标是正确分类，即为了找到将正负实例正确分开的分离超平面。为了找到这样的超平面,即确定感知机模型参数$\omega,b$，觉需要确定参数的评价指标，确定学习策略</p>
<ul>
<li>策略：定义（经验）损失函数（误分类点到超平面 S 的总距离），并将其极小化</li>
</ul>
<h4 id="感知机的损失函数"><a href="#感知机的损失函数" class="headerlink" title="感知机的损失函数"></a>感知机的损失函数</h4><ul>
<li>损失函数的一个自然选择是误分类点的总数，但是这样的损失函数不是参数$\omega,b$的连续可导函数，不易优化。</li>
</ul>
<p>（这里的详细理解要结合感知机损失函数的最优化方法——梯度下降法的原理来理解，详见<a href="">感知机进一步解释</a>）</p>
<ul>
<li><p>由上，我们选择误分类点到超平面 S 的总距离这一连续可导量作为损失函数</p>
</li>
<li><p>任意一点$x_0$到超平面 S 的距离：$\frac{1}{|\omega|}|\omega\cdot x_0 + b|$, 其中$|\omega|_2$是$\omega$的$L_2$范数</p>
</li>
<li><p>所有误分类的点记得集合为 M，所有误分类点到超平面 S 的总距离为：<br>$$L(\omega,b) &#x3D; - \frac{1}{|\omega|}\sum_{x_i \in M}y_i(\omega\cdot x_i + b)$$</p>
</li>
<li><p>损失函数：不考虑分母范数$\frac{1}{|\omega|}$，错误的点 $y_i(\omega\cdot x_i+b) &lt; b$，取距离为正，则感知机的损失函数（经验风险函数）为：<br>$$L(\omega,b) &#x3D; - \sum_{x_i \in M}y_i(\omega\cdot x_i + b)$$</p>
</li>
<li><p>显然损失函数$L(\omega,b)$是非负的。如果没有误分类点，损失函数值是 0。而且误分类点越少，误分类点离超平面越近，损失函数值就越小。</p>
</li>
<li><p>对于一个特定样本点的损失函数：在误分类时是参数$\omega,b$的线性函数，在正确分类时是 0。因此给定训练数据集 T，损失函数$L(\omega,b)$是$\omega,b$的连续可导函数</p>
</li>
<li><p>此时感知机学习问题转化为损失函数的最优化，即选择使上面损失函数最小的模型参数$\omega,b$</p>
</li>
</ul>
<h4 id="问题思考："><a href="#问题思考：" class="headerlink" title="问题思考："></a>问题思考：</h4><p><a href="">为什么能不考虑分母范数?<br>以距离作为损失函数很巧妙，但感知机能否还有其他损失函数？</a></p>
<hr>
<h6 id='4'></h6>

<h2 id="4-最初的感知机算法与其问题"><a href="#4-最初的感知机算法与其问题" class="headerlink" title="4.最初的感知机算法与其问题"></a>4.最初的感知机算法与其问题</h2><p>值得注意的是感知机学习规则、Hebb学习规则、以LMS算法、BP算法为代表的随机梯度下降法三者在应用在感知机模型上是形式上相似，加上现在感知机学习规则已不再被使用，一般书籍介绍都直接使用随机梯度下降法，故对于初学者容易混淆。实际上最初的感知机算法虽然采用迭代的方法，但没有采用随机梯度下降法，而使用感知机学习规则。这一点我觉得在《神经网络设计》这本书中讲解较为清楚，可参阅。</p>
<hr>
<h6 id='4.1'></h6>

<h3 id="4-1-最初感知机的学习思路"><a href="#4-1-最初感知机的学习思路" class="headerlink" title="4.1 最初感知机的学习思路"></a>4.1 最初感知机的学习思路</h3><p>发展了一种迭代、试错、类似于人类学习过程的学习算法——感知机学习。除了能够识别出现较多次的字母，感知机也能对不同书写方式的字母图像进行概括和归纳。</p>
<p>通过调整输出值的权重来学函数。对于每个实例，若感知机的输出值比实际值低太多，则增加它的权重若输出值比实际值高太多，则减小它的权重。</p>
<hr>
<h6 id='4.2'></h6>

<h3 id="4-2-最初的感知机算法"><a href="#4-2-最初的感知机算法" class="headerlink" title="4.2 最初的感知机算法"></a>4.2 最初的感知机算法</h3><p>基于感知机学习规则</p>
<ul>
<li>最原始状态</li>
</ul>
<ol>
<li>随机选择$\omega$和$b$</li>
<li>取一个训练样本$(x_i,y_i)$<br>若$\omega \cdots x+b&gt;0$且$y&#x3D;-1$，则：$$\omega&#x3D;\omega-x b&#x3D; b-1$$<br>若$\omega \cdots x+b&lt;0$且$y&#x3D;+1$，则：$$\omega&#x3D;\omega+x b&#x3D; b+1$$</li>
<li>再取另一个$(x_j,y_j)$，回到步骤 2</li>
<li>终止条件：直到所有输入输出对都不满足步骤 2 中两种情况，退出循环</li>
</ol>
<ul>
<li>在《机器学习》一书中也写作以下方式：<br>对训练样例$(x,y)$，若当前感知机的输出为$\hat{y}$，则感知机权重将这样调整：$$\begin{array}{l}\omega\leftarrow\omega_i+\Delta\omega_i,\\\Delta\omega_i&#x3D;\eta(y-\hat{y})x_i\end{array}$$其中，$\eta$称为学习率。从此算法可以看出，若感知机对训练样例$(x,y)$预测正确，即$\hat{y}&#x3D;y$，则感知机不发生变化，否则将根据错误的程度进行权重调整。</li>
</ul>
<hr>
<h6 id='4.3'></h6>

<h3 id="4-3-最初感知机算法的问题"><a href="#4-3-最初感知机算法的问题" class="headerlink" title="4.3 最初感知机算法的问题"></a>4.3 最初感知机算法的问题</h3><hr>
<h6 id='5'></h6>

<h2 id="5-自适性线性单元（ADALINE-与LMS算法——更加结合优化理论的模型"><a href="#5-自适性线性单元（ADALINE-与LMS算法——更加结合优化理论的模型" class="headerlink" title="5.自适性线性单元（ADALINE)与LMS算法——更加结合优化理论的模型"></a>5.自适性线性单元（ADALINE)与LMS算法——更加结合优化理论的模型</h2><p>罗森布莱特在阐述感知机时更强调其生物上的可解释性，而 1960 年，Bernard Widrow和Tedd Hoff探索的新模型——自适性线性单元在数学方法上对感知机进行了更深入的改进。<br>在自适性线性单元中，他们将阈值阶跃激活函数改为连续可导的线性函数，并引入均方误差损失函数和随机梯度下降法。这样的模型在数学上更美，因为<font color='red'>这样神经元的学习机制就变为了基于将误差最小化的微积分。</font></p>
<hr>
<h6 id='5.1'></h6>

<h3 id="5-1-自适性线性单元（ADALINE"><a href="#5-1-自适性线性单元（ADALINE" class="headerlink" title="5.1 自适性线性单元（ADALINE)"></a>5.1 自适性线性单元（ADALINE)</h3><p>这个输出是输入特征的线性组合，所以自适性线性单元解决的问题与线性回归解决的问题完全相同，但解决问题的方法完全不同：线性回归用最小二乘法求解，不太适合于计算机的求解：而自适性线性单元是基于detla法则（即随机梯度下降法）使用LMS算法迭代求解的。</p>
<hr>
<h3 id="delta学习法则"><a href="#delta学习法则" class="headerlink" title="delta学习法则"></a>delta学习法则</h3><p>delta法则的思想：基于训练集构造一个损失函数，将感知机的训练问题转化为求损失函数的最小化问题，从而利用梯度下降法等迭代方法去逐步逼近最佳参数。</p>
<h3 id="均方误差——损失函数的建立"><a href="#均方误差——损失函数的建立" class="headerlink" title="均方误差——损失函数的建立"></a>均方误差——损失函数的建立</h3><p>在样本集中，数据的标签是$y$，而模型预测值是$\hat{y}$，两者之间存在误差$y-\hat{y}$,而为了是模型预测准确，我们的目标自然是使误差最小，于是我们建立目标函数&#x2F;损失函数——均方误差$$E&#x3D;\frac{1}{2}\sum\limits_k(y_k-t_k)^2$$其中，$y_k$表示神经网络的输入，$t_k$表示监督数据，$k$表示数据的维数。</p>
<p>学习的目标就是使均方误差最小。</p>
<p>在统计学的贝叶斯推断中就涉及贝叶斯最小均方估计和线性最小均方估计，并且我们知道在所有基于X的$\Theta$的估计量$g(x)$中，条件期望$g(x)&#x3D;E(\Theta|X)$是使条件均方误差$E[(\Theta-\hat{\theta})^2|X&#x3D;x]$达到最小的量。同时，当$\Theta$的后验分布是关于（条件）均值对称并且是单峰的时（如正态分布），最大后验分布和条件期望估计量相同。而在经典统计推断中，我们称均方误差为残差平方和。</p>
<p>不同于统计学，在机器学习中我们更常用数值计算的方法（如梯度下降）求解最优解。</p>
<h3 id="为什么要建立损失函数的概念？——学习效果与模型性能需要评价指标"><a href="#为什么要建立损失函数的概念？——学习效果与模型性能需要评价指标" class="headerlink" title="为什么要建立损失函数的概念？——学习效果与模型性能需要评价指标"></a>为什么要建立损失函数的概念？——学习效果与模型性能需要评价指标</h3><p>”学习“是指训练数据中自动获取最优权重参数进而对测试数据做出更好预测的过程。为了评价模型的学习效果，使其正确的学习，就需要引入一个评价指标用来度量模型预测性能的好坏。只有评价指标建立了，学习的指导方向就确立了，进而才方便调整与改进。<br>因而我们引入损失函数和风险函数的概念。它们都是表示和评价模型性能的“恶劣程度”的指标。其中损失函数度量模型一次预测的好坏，风险函数度量平均意义下模型预测的好坏。<br>我们通过这些指标表示模型当前的学习状态，并以该指标为基准，进一步寻找最优权重参数。此时，学习问题就转化为损失函数的最优化问题，寻找使损失函数的值尽可能小的参数。</p>
<h3 id="3-为什么不是直接度量学习性能有多好而是有多坏？又为什么不计算失败个数"><a href="#3-为什么不是直接度量学习性能有多好而是有多坏？又为什么不计算失败个数" class="headerlink" title="3.为什么不是直接度量学习性能有多好而是有多坏？又为什么不计算失败个数"></a>3.为什么不是直接度量学习性能有多好而是有多坏？又为什么不计算失败个数</h3><p>为什么我们不直接以预测成功率或图像识别精度作为评价指标？成功率是以成功的实例的个数为丈量的，因而在训练数据的数量有限时，成功率的值只能以不连续的，离散的形式变化（类比于阶跃函数）。因而如果以其作为指标时，则参数的导数在绝大多数地方都会变成0。这将导致成功率对微小的参数变化基本没有什么反应（没有敏感性），即便有反应，它的值也是不连续地、突然变化的，不利于最优化学习。</p>
<h6 id='5.2'></h6>

<h3 id="5-2-梯度下降法（BGD"><a href="#5-2-梯度下降法（BGD" class="headerlink" title="5.2 梯度下降法（BGD)"></a>5.2 梯度下降法（BGD)</h3><p>根据微积分的知识：$\omega_{opt},b_{opt}$为函数$L(\omega,b)$的极值点的必要条件是$\eta L(\omega_{opt},b_{opt}))&#x3D;0$。所以，理论上可以通过求解这个方程来求无约束优化问题的最优解$\omega_{opt},b_{opt}$。然而，解方程不是计算机的长处，但计算机有强大的计算能力，可否通过算法一步步地去逼近问题的最优解$\omega_{opt},b_{opt}$?我们知道函数的梯度方向是函数值上升最快的方向，反方向就是函数值下降最快的方向了。因此，我们可以先给出$\omega,b$的一组初始值，然后沿着函数$L(\omega,b)$的负梯度方向不断地修改$\omega,b$的值，使函数值$L(\omega,b)$逐步逼近其最小值，这就是梯度下降法（BGD)的思想。</p>
<h3 id="对梯度的理解"><a href="#对梯度的理解" class="headerlink" title="对梯度的理解"></a>对梯度的理解</h3><p>机器学习的主要任务是在学习时寻找最优参数。这里的最优参数是指损失函数取最小值时的参数。然而损失函数很复杂，参数空间庞大，我们不知道它在何处能取得最小值。因而要寻找使损失函数的值尽可能小的地方，我们就需要借助导数来研究最值问题。因而需要计算参数的导数（确切地讲是梯度），然后以这个导数为指引，逐步更新参数。<br>此时，对权重参数的损失函数的求导，表示的是“如果稍微改变这个权重参数的值，损失函数的值会如何变化”。如果导数的值为负，通过该权重参数向正方向改变，可以减小损失函数的值；反过来，如果导数的值为正，则通过是该权重参数向负方向改变，可以减小损失函数的值。当导数的值为0时，无论权重参数向哪个方向变化，损失函数的值都不会改变，此时该权重参数的更新会停留在此处。</p>
<h3 id="为什么不采用解析求解？"><a href="#为什么不采用解析求解？" class="headerlink" title="为什么不采用解析求解？"></a>为什么不采用解析求解？</h3><p>正如前面所说线性回归可以使用最小二乘法直接解析求解。然而当特征数量比较大时，这种方法的计算量巨大，复杂度在$O(n^2)$以上（此处不详细说明）。所以我们采用迭代的方法。</p>
<h3 id="随机梯度下降法（SGD"><a href="#随机梯度下降法（SGD" class="headerlink" title="随机梯度下降法（SGD)"></a>随机梯度下降法（SGD)</h3><p>梯度下降法的每次迭代要遍历训练集的所有样本，如果训练集很大，计算量也将很大的。所以，实际上人们使用改进型的随机梯度下降法（Stochastic Gradient Desent,SGD）算法来训练。SGD算法每次迭代只计算一个样本，效率大大提升。由于样本的噪音和随机性，每次迭代并不一定按照函数值减少的方向。虽然存在一定的随机性，大量的迭代总体上沿着函数减少的方向前进，因此最后也能收敛到最小值附近。</p>
<p>随机梯度下降法的另一个优势是，当损失函数非常不规则时，随机梯度下降法其实可以帮助算法跳出局部最小值。所以相比批量随机梯度下降法，它对找到全局最小值更有优势，</p>
<h3 id="梯度下降法的问题"><a href="#梯度下降法的问题" class="headerlink" title="梯度下降法的问题"></a>梯度下降法的问题</h3><p>梯度表示的是各点处的函数值减小最多的方向。因此，无法保证梯度所指的方向就是函数的最小值或着真正应该前往的方向所在的方向。函数的极小值、最小值以及被称为鞍点的地方梯度为0。极小值是局部最小值。鞍点是从某个方向上看是极小值，从另一个方向上看则是极小值点。虽然梯度法是要寻找梯度为0的地方，但是那个地方不一定就是最小值（也可能是极小值或者鞍点）。<br>此外，当函数很复杂且呈扁平状时，学习可能会进入一个（几乎）平坦的地区，陷入被称为“学习高原”的无法前进的停滞期。</p>
<h3 id="LMS算法（Least-Mean-Square-最小均方）"><a href="#LMS算法（Least-Mean-Square-最小均方）" class="headerlink" title="LMS算法（Least Mean Square,最小均方）"></a>LMS算法（Least Mean Square,最小均方）</h3><p>LMS算法比感知机学习规则要强大得多。感知机规则能保证将训练模式收敛到一个可正确分类的解上，但得到的网络对噪声敏感，因为训练模式非常接近网络的判定边界。而LMS算法使均方误差最小化，从而使网络的判定边界尽量远离训练模式。</p>
<h4 id="LMS算法与原始感知机算法的比较"><a href="#LMS算法与原始感知机算法的比较" class="headerlink" title="LMS算法与原始感知机算法的比较"></a>LMS算法与原始感知机算法的比较</h4><p>Adaline与感知机很相似，两者具有相同的基本限制：它们只能对线性可分的数据集进行分类。然而，LMS算法仍比感知机学习规则更有效。因为它使均方误差最小化，所以算法能产生比感知机学习规则受噪声影响小的判定边界。</p>
<p>另外，LMS算法也因为它是反向传播BP算法的前驱而显得很重要。像BP算法也使用损失函数误差最小的随机梯度下降法。两算法唯一的区别在于导数的计算方式。BP算法是LMS算法的推广，可以用于多层感知机神经网络。而这些更复杂的网络能解决包括非线性问题在内的任意分类问题。</p>
<hr>
<h6 id='6'></h6>

<h2 id="6-借鉴自适性线性单元改进后的感知机算法——基于delta学习规则（随机梯度下降法）"><a href="#6-借鉴自适性线性单元改进后的感知机算法——基于delta学习规则（随机梯度下降法）" class="headerlink" title="6.借鉴自适性线性单元改进后的感知机算法——基于delta学习规则（随机梯度下降法）"></a>6.借鉴自适性线性单元改进后的感知机算法——基于delta学习规则（随机梯度下降法）</h2><h3 id="引入损失函数"><a href="#引入损失函数" class="headerlink" title="引入损失函数"></a>引入损失函数</h3><p>感知机学习算法是对于上述损失函数进行极小化，求得$\omega$和 b。这里使用随机梯度下降法(SGD)，因为误分类的 M 集合里面的样本才能参加损失函数的优化,即感知机算法是误分类驱动的。<br>目标函数如下:<br>$$L(\omega,b) &#x3D; arg\min_{\omega,b} \bigg(- \sum_{x_i \in M}y_i(\omega\cdot x_i + b) \bigg)$$<br>感知机学习算法有两种，一种是原始形式，一种是对偶形式，下面分别介绍:</p>
<hr>
<h6 id='6.1'></h6>

<h3 id="6-1-原始形式"><a href="#6-1-原始形式" class="headerlink" title="6.1 原始形式"></a>6.1 原始形式</h3><p>损失函数的最优化问题：误分类驱动，随机梯度下降法：</p>
<ul>
<li>损失函数的梯度<br>$$\nabla_\omega L(\omega,b) &#x3D; - \sum_{x_i \in M}y_i x_i$$<br>$$\nabla_b L(\omega,b) &#x3D; - \sum_{x_i \in M}y_i $$</li>
<li>给定$\eta(0 &lt; \eta \le 1)$作为步长(学习率)</li>
<li>目标：输出$\omega,b$,感知机模型$f(x) &#x3D; sign(\omega\cdot x_i + b)$<br>1.任意选取初值$\omega_0,b_0$<br>2.在训练集中随机选取数据（一个误分类点$(x_i,y_i)$)进行更新<br>3.如果$y_i(\omega\cdot x_i+b) \le b$,<br>$$\omega \leftarrow \omega + \eta y_i x_i$$$$b \leftarrow b + \eta y_i$$<br>式中的$\eta(0&lt;\eta\le1)$是步长，在统计学习中又称为 <strong><em>学习率</em></strong> 4.转到 2，直到没有误分类点，即损失函数减小为 0</li>
</ul>
<p>注意点:</p>
<ol>
<li>损失函数的极小化过程中不是一次使 M 中所有误分类点的梯度下降，而是一次随机选取一个误分类点是其梯度下降</li>
<li>感知机采用不同的初值或选取不同的误分类点，解可以不同</li>
</ol>
<p>####直观解释<br>当一个实例点被误分类,即位于分离超平面的错误一侧时,则调整$\omega ，b$的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类</p>
<p>####问题思考：<br><a href="">如何理解梯度，梯度下降法，随机梯度下降法，学习率?</a></p>
<hr>
<h6 id='6.2'></h6>

<h3 id="6-2-Novikoff-感知机算法的收敛证明"><a href="#6-2-Novikoff-感知机算法的收敛证明" class="headerlink" title="6.2 Novikoff 感知机算法的收敛证明:"></a>6.2 Novikoff 感知机算法的收敛证明:</h3><p>现在证明，对于线性可分数据集感知机学习算法原始形式收敛，即经过有限次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型。<br>为了便于叙述与推导，将偏置b并入权重向量$\omega$，记作$\hat{\omega}&#x3D;(\omega^T,b)^T$，同样也将输入向量加以扩充，加进常数1，记作$\hat{x}&#x3D;(x^T,1)^T$。这样，$\hat{x}\in R^{n+1},\hat{\omega}\in R^{n+1}$。显然，$\hat{omega}\cdot\hat{x}&#x3D;\omega\cdot x+b$。<br>感知机收敛定理：</p>
<ul>
<li>（1）存在满足条件 $|\hat{\omega}<em>{opt}|&#x3D;1$ 的超平面$\hat{\omega}</em>{opt}\cdot\hat{x}&#x3D;\omega_{opt}\cdot x+b_{opt}&#x3D;0$ 将训练数据集完全正确分开；且存在$\gamma&gt;0$，对所有$i&#x3D;1,2,\cdots,N$ $$y_i(\hat{\omega}\cdot\hat{x}<em>i)&#x3D;y_i(} \cdot x_i+b</em>{opt})\geqslant\gamma$$</li>
<li>（2）令$R&#x3D;\max\limits_{1\leqslant i\leqslant N}|\hat{x}_i|$，则感知机算法在训练数据集上的误分类次数$k$满足不等式 $$k\leqslant(\frac {R}{\gamma})^2$$</li>
</ul>
<p>证明：</p>
<ul>
<li><p>（1）由于训练数据集是线性可分的，按照线性可分的定义，存在超平面可将训练数据集完全正确分开，取此超平面为$\hat{\omega}<em>{opt}\cdot\hat{x}<em>i&#x3D;\omega</em>{opt}\cdot x_i+b</em>{opt}&#x3D;0$，使$|\hat{\omega}<em>{opt}|&#x3D;1$。由于对有限的$i&#x3D;1,2,\cdots,N$，均有$$y_i(\hat{\omega}\cdot\hat{x}<em>i)&#x3D;y_i(} \cdot x_i+b</em>{opt})&gt;0$$所以存在$$\gamma&#x3D;\min\limits_i\lbrace{y_i(\omega</em>{opt}\cdot x_i + b_{opt}\rbrace)}$$使$$\hat{\omega}<em>{opt}\cdot\hat{x}<em>i&#x3D;\omega</em>{opt}\cdot x_i+b</em>{opt}\geqslant\gamma$$</p>
</li>
<li><p>（2）感知机算法从$\hat{\omega}<em>0&#x3D;0$开始，如果实例被误分类，则更新权重。令$\hat{\omega}</em>{k-1}$是第$k$个误分类实例之前的扩充权重向量，即$$\hat{\omega}{k-1}&#x3D;(\omega^T_{k-1},b_{k-1})^T$$则第$k$个误分类实例的条件是$$\hat{\omega}<em>{k-1}\cdot\hat{x}_i&#x3D;\omega</em>{k-1}\cdot x_i+b_{opt}\leqslant0$$若$(x_i,y_i)$是被$\hat{\omega}<em>{k-1}&#x3D;(\omega^T</em>{k-1},b_{k-1})^T$误分类的数据，则$\omega$和$b$的更新是$$\omega_k\leftarrow\omega_{k-1}+\eta y_ix_i$$$$b_k\leftarrow b_{k-1} + \eta y_i$$即$$\hat{\omega}<em>k\leftarrow\hat{\omega}</em>{k-1}+\eta y_i\hat{x}_i$$<br>下面推导两个不等式：$$(1)\quad \hat{\omega}<em>k\cdot\hat{\omega}</em>{opt}\geqslant k\eta\gamma$$</p>
<p align="right"></p>
证明过程的有趣性：
证明中的一处比较有趣的地方在于，与学习算法的思路相同，数学证明时也采用迭代的方法。</li>
</ul>
<p>结论：</p>
<ul>
<li>误分类次数$k$有上界，经过有限次搜索可以找到将训练数据完全正确分开的超平面。</li>
<li>当训练数据集线性可分时，感知机学习算法原始形式迭代是收敛的。</li>
<li>当训练集线性不可分时，感知机学习算法不收敛，迭代结果会发生震荡。</li>
<li><font color="red">感知机学习算法存在许多解，既依赖于初值，也依赖于迭代过程中误分类点的选择顺序。为了得到唯一的超平面，需要对分离超平面增加约束条件。这就是第 7 章将要讲的<strong>线性支持向量机</strong>的想法。</font></li>
</ul>
<p>反思：学习不要被动接受死知识，而要主动提前去思考，当第一次得知感知机解的不唯一性时，我就应该意识到到这是一个问题并思考为什么会出现这种情况，如何优化与解决。这其实与上文求距离时不考虑分母范数的问题是相关联的，都是由感知机的所选择的损失函数所导致的</p>
<p>实际上在接触支持向量机相关内容后就会对现在的问题有比较清楚的理解</p>
<h3 id="判断样本集线性可分的充要条件"><a href="#判断样本集线性可分的充要条件" class="headerlink" title="判断样本集线性可分的充要条件"></a>判断样本集线性可分的充要条件</h3><p>证明以下定理：<strong>样本集线性可分的充分必要条件是正实例点所构成的凸壳与负实例点所构成的凸壳互不相交</strong>。</p>
<hr>
<h6 id='6.3'></h6>

<h3 id="6-3-对偶形式"><a href="#6-3-对偶形式" class="headerlink" title="6.3 对偶形式"></a>6.3 对偶形式</h3><p>基本想法：将$\omega,b$表示成实例$x_i$和标记$y_i$的线性组合形式。</p>
<ul>
<li>取初始值$\omega_0,b_0$均为 0</li>
<li>经过 n 次修改$\omega,b,\ \ \omega,b$关于$(x_i,y_i)$的增量分别是$\alpha_iy_ix_i$和$\alpha_iy_i$,这里$\alpha_i &#x3D; \eta_i\eta$，$\eta_i$是点$(x_i,y_i)$被误分类的次数</li>
<li>最后学到的$\omega,b$可表示成：<br>$$\omega &#x3D; \sum_{i&#x3D;1}^N\alpha_iy_ix_i$$$$b &#x3D; \sum_{i&#x3D;1}^N\alpha_i y_i$$</li>
<li>这里，$\alpha_i \ge 0,i&#x3D;1,2,3,\cdots,N$</li>
<li>当$\eta &#x3D; 1$时,$\alpha_i$表示第$i$个实例点由于误分类进行更新的次数，次数越多，意味着它距离分离超平面越近，很难正确分类，这样的实例对学习结果影响很大 ####对偶算法<br>目标：求$\alpha,b$,感知机模型$f(x) &#x3D; sign \bigg( \sum\limits_{j&#x3D;1}^N \alpha_jy_jx_j \cdot x+b\bigg)$，其中$\alpha &#x3D; (\alpha_1,\alpha_2,…,\alpha_N)^T$ 1.$\alpha &#x3D; 0, b &#x3D;0$ 2.选取训练集数据$(x_i,y_i)$ 3.如果$y_i \bigg( \sum\limits_{j&#x3D;1}^N \alpha_jy_jx_j \cdot x_i+b\bigg) \leq 0$<br>$$\alpha_i \leftarrow \alpha_i + \eta$$$$b \leftarrow b + \eta y_i$$ 4.转至 2，直到没有误分数据<br>对偶形式可以预先将训练集中的实例间的内积计算出来并以矩阵形式存储，该矩阵称为 Gram 矩阵$$ \mathbf G &#x3D; [x_i \cdot x_j]_{N \times N}$$</li>
</ul>
<hr>
<h6 id='6.4'></h6>

<h3 id="6-4-原始形式与对偶形式的简单比较与选择"><a href="#6-4-原始形式与对偶形式的简单比较与选择" class="headerlink" title="6.4 原始形式与对偶形式的简单比较与选择"></a>6.4 原始形式与对偶形式的简单比较与选择</h3><h4 id="产生两种形式的原因"><a href="#产生两种形式的原因" class="headerlink" title="产生两种形式的原因:"></a>产生两种形式的原因:</h4><ul>
<li>感知机之所以有两种形式，是因为采用的随机梯度下降，随机梯度下降每次迭代的是一个点，而不是整体，因此对于迭代的点有次数的概念。 ####选择与比较:</li>
<li>在向量维数（特征数）过高时，计算内积非常耗时，应选择对偶形式算法加速。</li>
<li>在向量个数（样本数）过多时，每次计算累计和就没有必要，应选择原始算法。</li>
</ul>
<h4 id="问题思考"><a href="#问题思考" class="headerlink" title="问题思考:"></a>问题思考:</h4><p>  <a href="">对于对偶形式的进一步思考</a></p>
<hr>
<h3 id="感知机的问题："><a href="#感知机的问题：" class="headerlink" title="感知机的问题："></a>感知机的问题：</h3><h4 id="梯度下降法的问题-局部最优与全局最优"><a href="#梯度下降法的问题-局部最优与全局最优" class="headerlink" title="梯度下降法的问题:局部最优与全局最优"></a>梯度下降法的问题:局部最优与全局最优</h4><h6 id='7'></h6>

<h2 id="7-算法实现"><a href="#7-算法实现" class="headerlink" title="7.算法实现"></a>7.算法实现</h2><hr>
<h6 id='7.1'></h6>

<h3 id="7-1-基于感知机-Perceptron-的鸢尾花分类实践"><a href="#7-1-基于感知机-Perceptron-的鸢尾花分类实践" class="headerlink" title="7.1 基于感知机 Perceptron 的鸢尾花分类实践"></a>7.1 基于感知机 Perceptron 的鸢尾花分类实践</h3><p>详见<a href="">算法实现</a></p>
<hr>
<h6 id='7.2'></h6>

<h3 id="7-2-基于感知机-Perceptron-的乳腺癌数据集分类"><a href="#7-2-基于感知机-Perceptron-的乳腺癌数据集分类" class="headerlink" title="7.2 基于感知机 Perceptron 的乳腺癌数据集分类"></a>7.2 基于感知机 Perceptron 的乳腺癌数据集分类</h3><p>详见<a href="">算法实现</a></p>
<hr>
<h6 id='8'></h6>

<h2 id="8-总结思考"><a href="#8-总结思考" class="headerlink" title="8.总结思考"></a>8.总结思考</h2><p>我们是如何认知的，是如何学习的，有哪些普适的底层方法？如果我们掌握了这些方法，我们能否使其他具有接受和处理信息及计算能力的事物如机器也掌握这些认知和学习方法。感知机在某种程度上简单思考了这个问题。</p>
<p>正如本学习笔记开篇引述的 1999 年图灵奖获得者弗雷德里克·布鲁克斯（Frederick P. Brooks）的这句话：“<strong>正确的判断来自于经验，而经验来自于错误的判断</strong>。”在生物神经元中，通过增加突触形成经验。而在感知算法中我们通过训练实例进行学习，并借助损失函数与随机梯度下降法迭代试错、更新参数来形成经验，进而进行正确的判断。</p>
<h3 id="8-1-“学习观的理解”和“数学建模”"><a href="#8-1-“学习观的理解”和“数学建模”" class="headerlink" title="8.1 “学习观的理解”和“数学建模”"></a>8.1 “学习观的理解”和“数学建模”</h3><p>作为神经网络和支持向量机的基础，理解感知机，我们可以从对“学习观的理解”和“数学建模”两个角度进行理解。</p>
<p>从“学习观的理解”角度，我们要理解归纳学习与逻辑演绎、从具体实例中学习、迭代试错学习、经验主义、感知机的生物学解释、机械学习硬式记忆与联结主义不同等。</p>
<p>从“数学建模”角度，我们要理解线性回归与最小二乘法背后的思想、规律的本质——函数、分类问题的本质——分离超平面&#x2F;超曲面、损失函数与随机梯度下降法、全局最优与局部最优、特征的数学化与向量表示等。</p>
<h3 id="8-2-理解感知机的具体三个层面"><a href="#8-2-理解感知机的具体三个层面" class="headerlink" title="8.2 理解感知机的具体三个层面"></a>8.2 理解感知机的具体三个层面</h3><h4 id="1-如何理解和处理“智能”——从学习能力到寻找超平面"><a href="#1-如何理解和处理“智能”——从学习能力到寻找超平面" class="headerlink" title="1.如何理解和处理“智能”——从学习能力到寻找超平面"></a>1.如何理解和处理“智能”——从学习能力到寻找超平面</h4><p>智能是一个抽象而空泛的概念，要想实现智能就必须将其具体化、实际化。首先，我们要认知到智能的核心与关键是<strong>学习能力</strong>。进一步，学习有各种方面，我们要意识到在人类生活中一个广泛而核心的学习能力是<strong>识别并归纳出事物规律的能力，即模式识别能力</strong>。再进行简化，我们可以尝试处理最简单的二元状态的模式识别问题，此时识别等价于<strong>一种判断问题，等价于二元分类问题</strong>。为了精确描述和解决问题，于是我们要设法引入数学语言，进行数学建模。借助于几何图像，我们发现二元分类问题可以归结于<strong>寻找一个超平面或超曲面</strong>。为了简化问题，感知机寻找的是超平面。</p>
<h4 id="2-从线性回归理解如何学习规律——从具体例子归纳学习"><a href="#2-从线性回归理解如何学习规律——从具体例子归纳学习" class="headerlink" title="2.从线性回归理解如何学习规律——从具体例子归纳学习"></a>2.从线性回归理解如何学习规律——从具体例子归纳学习</h4><p>既然问题变成了寻找超平面的数学问题，我们自然联想到统计数学中一个类似的问题——线性回归问题。在线性回归问题中，我们通过具体的样本点拟合一个线性函数，进而判断其他点相对于该线性函数的位置关系等。</p>
<p>线性回归给予我们如下启示：</p>
<ol>
<li>规律的本质是函数、映射关系。规律不是虚无的，规律无处不在并蕴含在具体实例中。</li>
<li>我们可以通过对具体事例和数据的归纳学习来得到函数，学习并掌握规律，而不必进行直接的解析式求解。</li>
</ol>
<p>感知机采用了这些观点，并大胆假设如果我们能够掌握大量例子，我们就能掌握规律本身。</p>
<h4 id="3-如何学习具体例子——联结主义的记忆与学习观——知识的储存与表示"><a href="#3-如何学习具体例子——联结主义的记忆与学习观——知识的储存与表示" class="headerlink" title="3.如何学习具体例子——联结主义的记忆与学习观——知识的储存与表示"></a>3.如何学习具体例子——联结主义的记忆与学习观——知识的储存与表示</h4><p>线性回归启示我们从具体例子学习规律的思路，然而我们仍然不知道如何学习具体例子。另一方面，统计学中的大数定理虽然印证着从具体例子学习的思路，但它也对训练事例的数量提出了要求，而我们应该如何从有限事例中总结出规律呢？</p>
<p>一种学习方法是机械式的学习，即将所有例子都存储起来，当遇到新事物时就与储存的例子进行比较，通过计算相似度来进行判断。且不论这种方法能否实现，这种方法的最大问题在于没有迁移能力，无法举一反三、随机应变。其实，我们可以发现本质上，这不是学习。从记忆的角度，这真的只是死记硬背。从信息的角度，这些信息虽然被储存，但并没有被处理、提取、综合，进而无法得到规律。</p>
<p>另一种思路联结主义。联结主义认为学习与记忆不是硬式存储，而是通过改变神经网络的连接结构与连接强度来实现的。这种从具体例子到连接强度的转化本身就是一种信息处理与提取的过程，进而整个神经网络的连接就是一个函数，是对规律的映射与表示。</p>
<p>Rosenblatt采取后一种观点来学习具体例子，因而他没有去储存和记忆具体例子，而是建立一个含未知参数的线性模型，并通过迭代试错的方法，用具体例子一个个地来调整未知参数，这样规律信息就从具体实例中转化、提取到参数中。而当学到的参数模型能够正确判断所有训练数据，我们就说它近似掌握了规律。进一步的，我们可以通过建立作为评价标准的损失函数并借助梯度下降法等数学方法来自动进行迭代试错，实现自动学习，而这就得到了感知机模型。</p>
<h3 id="8-3-感知机的美与局限"><a href="#8-3-感知机的美与局限" class="headerlink" title="8.3 感知机的美与局限"></a>8.3 感知机的美与局限</h3><p>我觉得感知机的美妙之处在于它是认知哲学与数学统计的巧妙结合，它虽然简陋，但却汇集了最核心的思想。它的每一处问题又恰恰引出机器学习的相关概念与后续发展。深度学习的基础模型——神经网络和统计学习的经典方法——支持向量机竟都起源于此，这本身就是十分有趣的。</p>
<p>感知机虽然能解决线性可分问题，然而它却无法处理非线性问题。同时感知机求出的分离超平面虽然是可行解，但却不是最优解。并且这种分离极易受噪声干扰，无法处理个别的特异点。</p>
<p>对我来说，学习感知机的一个最有趣的体验是认知成长的过程。在学习最初我对感知机完全不知所云，逐渐了解其原理、历史与思想后，我为其背后的美妙认知所惊叹，然而在进一步学习其他模型后我又认知到感知机的浅显。感知机的思想对于一个初学者已足够美妙，然而事实证明自然世界是深不可测的，感知机背后思想的美只是大自然无限魅力的渺小的一部分。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://homeofzhixiang.github.io">沧月倾</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://homeofzhixiang.github.io/2023/01/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%84%9F%E7%9F%A5%E6%9C%BA-%E6%84%9F%E7%9F%A5%E6%9C%BA/">http://homeofzhixiang.github.io/2023/01/24/机器学习-感知机-感知机/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://homeofzhixiang.github.io" target="_blank">Homeofzhixiang</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="/img/cover/2.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-full"><a href="/2023/01/23/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0-2-1-1-BFS%E4%B8%AD%E7%9A%84Flood-Fill%E5%92%8C%E6%9C%80%E7%9F%AD%E8%B7%AF%E6%A8%A1%E5%9E%8B/" title="算法提高课笔记——2.1.1 BFS中的Flood Fill和最短路模型"><img class="cover" src="/img/cover/12-1.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">算法提高课笔记——2.1.1 BFS中的Flood Fill和最短路模型</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar/1.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">沧月倾</div><div class="author-info__description">枯萎的花儿，能否再次绽放</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">3</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/homeofzhixiang"><i class="fab fa-github"></i><span>🛴前往小家...</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content"><center>Welcome to ZX's blog!🍭🍭🍭</center></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%88Perceptron-%E4%B8%8E%E8%87%AA%E9%80%82%E6%80%A7%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83%EF%BC%88Adaline-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0"><span class="toc-number">1.</span> <span class="toc-text">感知机（Perceptron)与自适性线性单元（Adaline)学习笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#1"><span class="toc-number">1.0.0.0.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%80%E4%BB%8B"><span class="toc-number">1.1.</span> <span class="toc-text">1.感知机简介</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.1.0.1.</span> <span class="toc-text">维基百科介绍:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%AE%80%E8%BF%B0%EF%BC%9A"><span class="toc-number">1.1.0.2.</span> <span class="toc-text">模型简述：</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#2"><span class="toc-number">1.1.0.2.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%8E%86%E5%8F%B2"><span class="toc-number">1.2.</span> <span class="toc-text">2. 感知机的历史</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%EF%BC%9F"><span class="toc-number">1.2.1.</span> <span class="toc-text">如何学习？</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#3"><span class="toc-number">1.2.1.0.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E5%AF%B9%E8%A7%84%E5%BE%8B%E7%9A%84%E8%AE%A4%E7%9F%A5"><span class="toc-number">1.3.</span> <span class="toc-text">3. 线性回归与对规律的认知</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E2%80%94%E2%80%94%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC%E6%9C%BA-x2F-%E9%80%BC%E8%BF%91%E5%99%A8"><span class="toc-number">1.3.1.</span> <span class="toc-text">线性回归——函数近似机&#x2F;逼近器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%B1%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%80%9D%E8%80%83%E4%BD%95%E4%B8%BA%E8%A7%84%E5%BE%8B%EF%BC%9F"><span class="toc-number">1.3.2.</span> <span class="toc-text">由线性回归思考何为规律？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E8%A7%84%E5%BE%8B%E6%80%A7%E5%AD%98%E5%9C%A8%E7%9A%84%E5%81%87%E8%AE%BE"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">对规律性存在的假设</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%84%E5%BE%8B%E7%9A%84%E6%9C%AC%E8%B4%A8"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">规律的本质</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%84%E5%BE%8B%E7%9A%84%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.3.2.3.</span> <span class="toc-text">规律的学习</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#4"><span class="toc-number">1.3.2.3.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E8%A7%92%E5%BA%A6%E7%9A%84%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%8E%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">1.4.</span> <span class="toc-text">数学角度的分类问题与感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E8%AE%A4%E7%9F%A5"><span class="toc-number">1.4.1.</span> <span class="toc-text">对分类问题的认知</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E8%A7%92%E5%BA%A6%E7%9A%84%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-number">1.4.2.</span> <span class="toc-text">数学角度的分类问题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%95%E4%B8%BA%E5%87%BD%E6%95%B0"><span class="toc-number">1.4.2.1.</span> <span class="toc-text">何为函数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1"><span class="toc-number">1.4.3.</span> <span class="toc-text">数学建模</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E8%A7%92%E5%BA%A6%E7%90%86%E8%A7%A3%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">1.4.4.</span> <span class="toc-text">从数学建模角度理解感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#4"><span class="toc-number">1.4.4.0.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-MP-%E7%A5%9E%E7%BB%8F%E5%85%83%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.5.</span> <span class="toc-text">4. MP 神经元模型——逻辑运算的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8E%9F%E5%A7%8B%E7%9A%84-MP-%E7%A5%9E%E7%BB%8F%E5%85%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.5.0.1.</span> <span class="toc-text">原始的 MP 神经元模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8E%9F%E5%A7%8B-MP-%E7%A5%9E%E7%BB%8F%E5%85%83%E7%9A%84%E9%97%AE%E9%A2%98%E2%80%94%E2%80%94%E6%9D%83%E9%87%8D%E7%9A%84%E7%BC%BA%E5%A4%B1"><span class="toc-number">1.5.0.2.</span> <span class="toc-text">原始 MP 神经元的问题——权重的缺失</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%95%E5%85%A5%E6%9D%83%E9%87%8D%E6%A6%82%E5%BF%B5%E7%9A%84-MP-%E7%A5%9E%E7%BB%8F%E5%85%83"><span class="toc-number">1.5.0.3.</span> <span class="toc-text">引入权重概念的 MP 神经元</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MP-%E7%A5%9E%E7%BB%8F%E5%85%83%E7%9A%84%E9%97%AE%E9%A2%98%E2%80%94%E2%80%94%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%E5%B9%B6%E4%B8%8D%E8%83%BD%E5%AE%9E%E7%8E%B0%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.5.1.</span> <span class="toc-text">MP 神经元的问题——逻辑运算并不能实现学习</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#5"><span class="toc-number">1.5.1.0.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E8%81%94%E7%BB%93%E4%B8%BB%E4%B9%89%E5%AF%B9%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%90%86%E8%A7%A3"><span class="toc-number">1.6.</span> <span class="toc-text">5. 如何学习——联结主义对学习的理解</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A1%91%E4%BB%A3%E5%85%8B%E7%9A%84%E7%A0%94%E7%A9%B6"><span class="toc-number">1.6.0.1.</span> <span class="toc-text">桑代克的研究</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B5%AB%E5%B8%83%E8%A7%84%E5%88%99"><span class="toc-number">1.6.0.2.</span> <span class="toc-text">赫布规则</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B5%AB%E5%B8%83%E8%A7%84%E5%88%99%E7%9A%84%E5%90%AF%E7%A4%BA"><span class="toc-number">1.6.0.3.</span> <span class="toc-text">赫布规则的启示</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%A4%E7%9F%A5%E5%BF%83%E7%90%86%E5%AD%A6%E4%B8%AD%E7%9A%84%E8%81%94%E7%BB%93%E4%B8%BB%E4%B9%89"><span class="toc-number">1.6.0.4.</span> <span class="toc-text">认知心理学中的联结主义</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#5"><span class="toc-number">1.6.0.4.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Rosenblatt-%E5%AF%B9%E8%AE%B0%E5%BF%86%E4%B8%8E%E6%84%9F%E7%9F%A5%E7%9A%84%E7%90%86%E8%A7%A3"><span class="toc-number">1.7.</span> <span class="toc-text">5. Rosenblatt 对记忆与感知的理解</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E%E8%AE%B0%E5%BF%86%E4%B8%8E%E6%84%9F%E7%9F%A5%E7%9A%84%E4%B8%A4%E7%A7%8D%E8%A7%82%E7%82%B9%E2%80%94%E2%80%94%E2%80%94%E2%80%94%E7%A1%AC%E5%BC%8F%E7%BC%96%E7%A0%81%E4%B8%8E%E8%81%94%E7%BB%93%E4%B8%BB%E4%B9%89"><span class="toc-number">1.7.0.1.</span> <span class="toc-text">关于记忆与感知的两种观点————硬式编码与联结主义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Rosenblatt-%E7%9A%84%E8%A7%82%E7%82%B9"><span class="toc-number">1.7.0.2.</span> <span class="toc-text">Rosenblatt 的观点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%87%87%E7%94%A8%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E8%80%8C%E9%9D%9E%E7%AC%A6%E5%8F%B7%E9%80%BB%E8%BE%91"><span class="toc-number">1.7.0.3.</span> <span class="toc-text">采用概率语言而非符号逻辑</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#6"><span class="toc-number">1.7.0.3.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%AD%A6%E4%B9%A0%E8%A7%82"><span class="toc-number">1.8.</span> <span class="toc-text">6. 感知机的学习观</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA%E8%83%8C%E5%90%8E%E6%B6%89%E5%8F%8A%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95"><span class="toc-number">1.8.1.</span> <span class="toc-text">感知机背后涉及的一些学习方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BD%92%E7%BA%B3%E6%8E%A8%E7%90%86%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E4%BE%8B%E5%AD%90%E7%9A%84%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.8.1.1.</span> <span class="toc-text">归纳推理——基于例子的学习</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%95%E9%94%99%E6%B3%95%E2%80%94%E2%80%94%E9%80%90%E6%AD%A5%E8%B0%83%E6%95%B4"><span class="toc-number">1.8.1.2.</span> <span class="toc-text">试错法——逐步调整</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%AD%E4%BB%A3%E6%B3%95%E2%80%94%E2%80%94%E9%87%8D%E5%A4%8D%E6%89%A7%E8%A1%8C%EF%BC%8C%E9%80%90%E6%AD%A5%E4%BC%98%E5%8C%96"><span class="toc-number">1.8.1.3.</span> <span class="toc-text">迭代法——重复执行，逐步优化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%A4%A7%E8%83%86%E5%81%87%E8%AE%BE"><span class="toc-number">1.8.2.</span> <span class="toc-text">感知机的大胆假设</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA%E4%B8%8E%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">1.8.3.</span> <span class="toc-text">感知机与传统方法的区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%95%E9%94%99%E7%9A%84%E6%96%B9%E6%B3%95%E4%B8%8E%E8%AF%95%E9%94%99%E7%9A%84%E6%95%88%E7%8E%87%E9%97%AE%E9%A2%98"><span class="toc-number">1.8.4.</span> <span class="toc-text">试错的方法与试错的效率问题</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#7"><span class="toc-number">1.8.4.0.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E6%84%9F%E7%9F%A5%E6%9C%BA%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.9.</span> <span class="toc-text">7.感知机模型</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#3"><span class="toc-number">1.9.0.0.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%84%9F%E7%9F%A5%E6%9C%BA%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5"><span class="toc-number">1.10.</span> <span class="toc-text">3.感知机学习策略</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E8%A1%8C%EF%BC%9A"><span class="toc-number">1.10.0.1.</span> <span class="toc-text">数据集的线性可分行：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5"><span class="toc-number">1.10.0.2.</span> <span class="toc-text">感知机的学习策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.10.0.3.</span> <span class="toc-text">感知机的损失函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E6%80%9D%E8%80%83%EF%BC%9A"><span class="toc-number">1.10.0.4.</span> <span class="toc-text">问题思考：</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#4"><span class="toc-number">1.10.0.4.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%9C%80%E5%88%9D%E7%9A%84%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95%E4%B8%8E%E5%85%B6%E9%97%AE%E9%A2%98"><span class="toc-number">1.11.</span> <span class="toc-text">4.最初的感知机算法与其问题</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#4.1"><span class="toc-number">1.11.0.0.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E6%9C%80%E5%88%9D%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%B7%AF"><span class="toc-number">1.11.1.</span> <span class="toc-text">4.1 最初感知机的学习思路</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#4.2"><span class="toc-number">1.11.1.0.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E6%9C%80%E5%88%9D%E7%9A%84%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95"><span class="toc-number">1.11.2.</span> <span class="toc-text">4.2 最初的感知机算法</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#4.3"><span class="toc-number">1.11.2.0.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E6%9C%80%E5%88%9D%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">1.11.3.</span> <span class="toc-text">4.3 最初感知机算法的问题</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#5"><span class="toc-number">1.11.3.0.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E8%87%AA%E9%80%82%E6%80%A7%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83%EF%BC%88ADALINE-%E4%B8%8ELMS%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E6%9B%B4%E5%8A%A0%E7%BB%93%E5%90%88%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.12.</span> <span class="toc-text">5.自适性线性单元（ADALINE)与LMS算法——更加结合优化理论的模型</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#5.1"><span class="toc-number">1.12.0.0.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E8%87%AA%E9%80%82%E6%80%A7%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83%EF%BC%88ADALINE"><span class="toc-number">1.12.1.</span> <span class="toc-text">5.1 自适性线性单元（ADALINE)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#delta%E5%AD%A6%E4%B9%A0%E6%B3%95%E5%88%99"><span class="toc-number">1.12.2.</span> <span class="toc-text">delta学习法则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE%E2%80%94%E2%80%94%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%BB%BA%E7%AB%8B"><span class="toc-number">1.12.3.</span> <span class="toc-text">均方误差——损失函数的建立</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%BB%BA%E7%AB%8B%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E6%A6%82%E5%BF%B5%EF%BC%9F%E2%80%94%E2%80%94%E5%AD%A6%E4%B9%A0%E6%95%88%E6%9E%9C%E4%B8%8E%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E9%9C%80%E8%A6%81%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="toc-number">1.12.4.</span> <span class="toc-text">为什么要建立损失函数的概念？——学习效果与模型性能需要评价指标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E6%98%AF%E7%9B%B4%E6%8E%A5%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0%E6%80%A7%E8%83%BD%E6%9C%89%E5%A4%9A%E5%A5%BD%E8%80%8C%E6%98%AF%E6%9C%89%E5%A4%9A%E5%9D%8F%EF%BC%9F%E5%8F%88%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E8%AE%A1%E7%AE%97%E5%A4%B1%E8%B4%A5%E4%B8%AA%E6%95%B0"><span class="toc-number">1.12.5.</span> <span class="toc-text">3.为什么不是直接度量学习性能有多好而是有多坏？又为什么不计算失败个数</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#5.2"><span class="toc-number">1.12.5.0.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%88BGD"><span class="toc-number">1.12.6.</span> <span class="toc-text">5.2 梯度下降法（BGD)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E6%A2%AF%E5%BA%A6%E7%9A%84%E7%90%86%E8%A7%A3"><span class="toc-number">1.12.7.</span> <span class="toc-text">对梯度的理解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E9%87%87%E7%94%A8%E8%A7%A3%E6%9E%90%E6%B1%82%E8%A7%A3%EF%BC%9F"><span class="toc-number">1.12.8.</span> <span class="toc-text">为什么不采用解析求解？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%88SGD"><span class="toc-number">1.12.9.</span> <span class="toc-text">随机梯度下降法（SGD)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">1.12.10.</span> <span class="toc-text">梯度下降法的问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LMS%E7%AE%97%E6%B3%95%EF%BC%88Least-Mean-Square-%E6%9C%80%E5%B0%8F%E5%9D%87%E6%96%B9%EF%BC%89"><span class="toc-number">1.12.11.</span> <span class="toc-text">LMS算法（Least Mean Square,最小均方）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#LMS%E7%AE%97%E6%B3%95%E4%B8%8E%E5%8E%9F%E5%A7%8B%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-number">1.12.11.1.</span> <span class="toc-text">LMS算法与原始感知机算法的比较</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#6"><span class="toc-number">1.12.11.1.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E5%80%9F%E9%89%B4%E8%87%AA%E9%80%82%E6%80%A7%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83%E6%94%B9%E8%BF%9B%E5%90%8E%E7%9A%84%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8Edelta%E5%AD%A6%E4%B9%A0%E8%A7%84%E5%88%99%EF%BC%88%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%89"><span class="toc-number">1.13.</span> <span class="toc-text">6.借鉴自适性线性单元改进后的感知机算法——基于delta学习规则（随机梯度下降法）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%95%E5%85%A5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.13.1.</span> <span class="toc-text">引入损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#6.1"><span class="toc-number">1.13.1.0.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E5%8E%9F%E5%A7%8B%E5%BD%A2%E5%BC%8F"><span class="toc-number">1.13.2.</span> <span class="toc-text">6.1 原始形式</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#6.2"><span class="toc-number">1.13.2.0.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-Novikoff-%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95%E7%9A%84%E6%94%B6%E6%95%9B%E8%AF%81%E6%98%8E"><span class="toc-number">1.13.3.</span> <span class="toc-text">6.2 Novikoff 感知机算法的收敛证明:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%A4%E6%96%AD%E6%A0%B7%E6%9C%AC%E9%9B%86%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E7%9A%84%E5%85%85%E8%A6%81%E6%9D%A1%E4%BB%B6"><span class="toc-number">1.13.4.</span> <span class="toc-text">判断样本集线性可分的充要条件</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#6.3"><span class="toc-number">1.13.4.0.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-%E5%AF%B9%E5%81%B6%E5%BD%A2%E5%BC%8F"><span class="toc-number">1.13.5.</span> <span class="toc-text">6.3 对偶形式</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#6.4"><span class="toc-number">1.13.5.0.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-%E5%8E%9F%E5%A7%8B%E5%BD%A2%E5%BC%8F%E4%B8%8E%E5%AF%B9%E5%81%B6%E5%BD%A2%E5%BC%8F%E7%9A%84%E7%AE%80%E5%8D%95%E6%AF%94%E8%BE%83%E4%B8%8E%E9%80%89%E6%8B%A9"><span class="toc-number">1.13.6.</span> <span class="toc-text">6.4 原始形式与对偶形式的简单比较与选择</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%A7%E7%94%9F%E4%B8%A4%E7%A7%8D%E5%BD%A2%E5%BC%8F%E7%9A%84%E5%8E%9F%E5%9B%A0"><span class="toc-number">1.13.6.1.</span> <span class="toc-text">产生两种形式的原因:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E6%80%9D%E8%80%83"><span class="toc-number">1.13.6.2.</span> <span class="toc-text">问题思考:</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%9A"><span class="toc-number">1.13.7.</span> <span class="toc-text">感知机的问题：</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%9A%84%E9%97%AE%E9%A2%98-%E5%B1%80%E9%83%A8%E6%9C%80%E4%BC%98%E4%B8%8E%E5%85%A8%E5%B1%80%E6%9C%80%E4%BC%98"><span class="toc-number">1.13.7.1.</span> <span class="toc-text">梯度下降法的问题:局部最优与全局最优</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#7"><span class="toc-number">1.13.7.1.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.14.</span> <span class="toc-text">7.算法实现</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#7.1"><span class="toc-number">1.14.0.0.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-%E5%9F%BA%E4%BA%8E%E6%84%9F%E7%9F%A5%E6%9C%BA-Perceptron-%E7%9A%84%E9%B8%A2%E5%B0%BE%E8%8A%B1%E5%88%86%E7%B1%BB%E5%AE%9E%E8%B7%B5"><span class="toc-number">1.14.1.</span> <span class="toc-text">7.1 基于感知机 Perceptron 的鸢尾花分类实践</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#7.2"><span class="toc-number">1.14.1.0.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-%E5%9F%BA%E4%BA%8E%E6%84%9F%E7%9F%A5%E6%9C%BA-Perceptron-%E7%9A%84%E4%B9%B3%E8%85%BA%E7%99%8C%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%86%E7%B1%BB"><span class="toc-number">1.14.2.</span> <span class="toc-text">7.2 基于感知机 Perceptron 的乳腺癌数据集分类</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#8"><span class="toc-number">1.14.2.0.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E6%80%BB%E7%BB%93%E6%80%9D%E8%80%83"><span class="toc-number">1.15.</span> <span class="toc-text">8.总结思考</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-%E2%80%9C%E5%AD%A6%E4%B9%A0%E8%A7%82%E7%9A%84%E7%90%86%E8%A7%A3%E2%80%9D%E5%92%8C%E2%80%9C%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E2%80%9D"><span class="toc-number">1.15.1.</span> <span class="toc-text">8.1 “学习观的理解”和“数学建模”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-%E7%90%86%E8%A7%A3%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%85%B7%E4%BD%93%E4%B8%89%E4%B8%AA%E5%B1%82%E9%9D%A2"><span class="toc-number">1.15.2.</span> <span class="toc-text">8.2 理解感知机的具体三个层面</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E5%92%8C%E5%A4%84%E7%90%86%E2%80%9C%E6%99%BA%E8%83%BD%E2%80%9D%E2%80%94%E2%80%94%E4%BB%8E%E5%AD%A6%E4%B9%A0%E8%83%BD%E5%8A%9B%E5%88%B0%E5%AF%BB%E6%89%BE%E8%B6%85%E5%B9%B3%E9%9D%A2"><span class="toc-number">1.15.2.1.</span> <span class="toc-text">1.如何理解和处理“智能”——从学习能力到寻找超平面</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BB%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%90%86%E8%A7%A3%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%E8%A7%84%E5%BE%8B%E2%80%94%E2%80%94%E4%BB%8E%E5%85%B7%E4%BD%93%E4%BE%8B%E5%AD%90%E5%BD%92%E7%BA%B3%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.15.2.2.</span> <span class="toc-text">2.从线性回归理解如何学习规律——从具体例子归纳学习</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%E5%85%B7%E4%BD%93%E4%BE%8B%E5%AD%90%E2%80%94%E2%80%94%E8%81%94%E7%BB%93%E4%B8%BB%E4%B9%89%E7%9A%84%E8%AE%B0%E5%BF%86%E4%B8%8E%E5%AD%A6%E4%B9%A0%E8%A7%82%E2%80%94%E2%80%94%E7%9F%A5%E8%AF%86%E7%9A%84%E5%82%A8%E5%AD%98%E4%B8%8E%E8%A1%A8%E7%A4%BA"><span class="toc-number">1.15.2.3.</span> <span class="toc-text">3.如何学习具体例子——联结主义的记忆与学习观——知识的储存与表示</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-3-%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E7%BE%8E%E4%B8%8E%E5%B1%80%E9%99%90"><span class="toc-number">1.15.3.</span> <span class="toc-text">8.3 感知机的美与局限</span></a></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 By 沧月倾</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><canvas id="universe"></canvas><script defer src="/js/universe.js"></script><script defer src="https://cdn.jsdelivr.net/gh/homeofzhixiang/live2d-widget/autoload.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="30" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"]):not([href="/music/"]):not([href="/no-pjax/"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://homeofzhixiang.github.io/categories/算法笔记/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 蒟蒻の算法笔记 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://homeofzhixiang.github.io/categories/record/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱‍👓 小冰の随笔 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://homeofzhixiang.github.io/categories/机器学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">👩‍💻 机器学习 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item" style="visibility: hidden"></div><a class="magnet_link_more"  href="http://homeofzhixiang.github.io/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #69e8f2}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>
  function butterfly_clock_anzhiyu_injector_config(){
    var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
    var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img class="entered loading" id="card-clock-loading" src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading"/></div></div></div></div></div>';
    console.log('已挂载butterfly_clock_anzhiyu')
    if(parent_div_git) {
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var qweather_key = 'f1b52487749c4e358d7f84ab68914874';
  var gaud_map_key = 'e2b04289e870b005374ee030148d64fd&s=rsv3';
  var baidu_ak_key = 'undefined';
  var flag = 0;
  var clock_rectangle = '113.34532,23.15624';
  var clock_default_rectangle_enable = 'false';

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_clock_anzhiyu_injector_config();
  }
  else if (epage === cpage){
    butterfly_clock_anzhiyu_injector_config();
  }
  </script><script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script><script data-pjax src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.js"></script><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v6.2.0" title=""><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="主题版本Butterfly_v4.3.1" title=""><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://vercel.com/" style="margin-inline:5px" data-title="本站采用多线部署，主线路托管于Vercel" title=""><img src="https://img.shields.io/badge/Hosted-Vercel-brightgreen?style=flat&amp;logo=Vercel" alt=""/></a><a class="github-badge" target="_blank" href="https://dashboard.4everland.org/" style="margin-inline:5px" data-title="本站采用多线部署，备用线路托管于4EVERLAND" title=""><img src="https://img.shields.io/badge/Hosted-4EVERLAND-22DDDD?style=flat&amp;logo=IPFS" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title=""><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title=""><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" alt=""/></a></p>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script async src="/js/runtime.js"></script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '2s');
    arr[i].setAttribute('data-wow-delay', '1s');
    arr[i].setAttribute('data-wow-offset', '100');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow_init.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2023/01/23/hello-world/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src="/img/cover/6.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-01-23</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2023/01/23/hello-world/&quot;);" href="javascript:void(0);" alt="">Hello World</a><div class="blog-slider__text">Welcome to my blog!</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2023/01/23/hello-world/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;2023/01/23/算法笔记-2-1-1-BFS中的Flood-Fill和最短路模型/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src="/img/cover/12-1.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-01-23</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;2023/01/23/算法笔记-2-1-1-BFS中的Flood-Fill和最短路模型/&quot;);" href="javascript:void(0);" alt="">算法提高课笔记——2.1.1 BFS中的Flood Fill和最短路模型</a><div class="blog-slider__text">BFS</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;2023/01/23/算法笔记-2-1-1-BFS中的Flood-Fill和最短路模型/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --></body></html>